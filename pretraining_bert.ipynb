{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-training BERT\n",
    "preprocess_bert.py 에서 이어짐. <br/>\n",
    "아래 링크에서 step.6 부터\n",
    "https://colab.research.google.com/drive/1nVn6AFpQSzXBt8_ywfx6XR8ZfQXlKGAz#scrollTo=myjxQe5awo1v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "sys.path.append(\"bert\")\n",
    "\n",
    "from bert import modeling, optimization, tokenization\n",
    "from bert.run_pretraining import input_fn_builder, model_fn_builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure logging\n",
    "log = logging.getLogger('tensorflow')\n",
    "log.setLevel(logging.INFO)\n",
    "\n",
    "# create formatter and add it to the handlers\n",
    "formatter = logging.Formatter('%(asctime)s :  %(message)s')\n",
    "sh = logging.StreamHandler()\n",
    "sh.setLevel(logging.INFO)\n",
    "sh.setFormatter(formatter)\n",
    "log.handlers = [sh]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOC_FNAME = \"vocab.txt\" #@param {type:\"string\"}\n",
    "VOC_SIZE = 32000 #@param {type:\"integer\"}\n",
    "USE_TPU=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"bert_resourses\" #@param {type:\"string\"}\n",
    "MODEL_DIR = \"bert_model\" #@param {type:\"string\"}\n",
    "tf.gfile.MkDir(MODEL_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hyperparameter 세팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this for BERT-base\n",
    "\n",
    "bert_base_config = {\n",
    "  \"attention_probs_dropout_prob\": 0.1, \n",
    "  \"directionality\": \"bidi\", \n",
    "  \"hidden_act\": \"gelu\", \n",
    "  \"hidden_dropout_prob\": 0.1, \n",
    "  \"hidden_size\": 768, \n",
    "  \"initializer_range\": 0.02, \n",
    "  \"intermediate_size\": 3072, \n",
    "  \"max_position_embeddings\": 512, \n",
    "  \"num_attention_heads\": 12, \n",
    "  \"num_hidden_layers\": 12, \n",
    "  \"pooler_fc_size\": 768, \n",
    "  \"pooler_num_attention_heads\": 12, \n",
    "  \"pooler_num_fc_layers\": 3, \n",
    "  \"pooler_size_per_head\": 128, \n",
    "  \"pooler_type\": \"first_token_transform\", \n",
    "  \"type_vocab_size\": 2, \n",
    "  \"vocab_size\": VOC_SIZE\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-31 12:32:39,379 :  Using checkpoint: None\n",
      "2019-12-31 12:32:39,385 :  Using 4 data shards\n"
     ]
    }
   ],
   "source": [
    "BUCKET_NAME = \"bert_resourses\" #@param {type:\"string\"}\n",
    "MODEL_DIR = \"bert_model\" #@param {type:\"string\"}\n",
    "PRETRAINING_DIR = \"pretraining_data\" #@param {type:\"string\"}\n",
    "VOC_FNAME = \"vocab.txt\" #@param {type:\"string\"}\n",
    "\n",
    "# Input data pipeline config\n",
    "TRAIN_BATCH_SIZE = 128 #@param {type:\"integer\"}\n",
    "MAX_PREDICTIONS = 20 #@param {type:\"integer\"}\n",
    "MAX_SEQ_LENGTH = 128 #@param {type:\"integer\"}\n",
    "MASKED_LM_PROB = 0.15 #@param\n",
    "\n",
    "# Training procedure config\n",
    "EVAL_BATCH_SIZE = 64\n",
    "LEARNING_RATE = 2e-5\n",
    "TRAIN_STEPS = 1000000 #@param {type:\"integer\"}\n",
    "SAVE_CHECKPOINTS_STEPS = 2500 #@param {type:\"integer\"}\n",
    "NUM_TPU_CORES = 8\n",
    "\n",
    "BUCKET_PATH = \".\"\n",
    "\n",
    "BERT_GCS_DIR = \"{}/{}\".format(BUCKET_PATH, MODEL_DIR)\n",
    "DATA_GCS_DIR = \"{}/{}\".format(BUCKET_PATH, PRETRAINING_DIR)\n",
    "\n",
    "VOCAB_FILE = os.path.join(BERT_GCS_DIR, VOC_FNAME)\n",
    "CONFIG_FILE = os.path.join(BERT_GCS_DIR, \"bert_config.json\")\n",
    "\n",
    "INIT_CHECKPOINT = tf.train.latest_checkpoint(BERT_GCS_DIR)\n",
    "\n",
    "bert_config = modeling.BertConfig.from_json_file(CONFIG_FILE)\n",
    "input_files = tf.gfile.Glob(os.path.join(DATA_GCS_DIR,'*tfrecord'))\n",
    "\n",
    "log.info(\"Using checkpoint: {}\".format(INIT_CHECKPOINT))\n",
    "log.info(\"Using {} data shards\".format(len(input_files)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fn = model_fn_builder(\n",
    "      bert_config=bert_config,\n",
    "      init_checkpoint=INIT_CHECKPOINT,\n",
    "      learning_rate=LEARNING_RATE,\n",
    "      num_train_steps=TRAIN_STEPS,\n",
    "      num_warmup_steps=10,\n",
    "      use_tpu=USE_TPU,\n",
    "      use_one_hot_embeddings=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# model_fn_builder code review\n",
    "bert.run_pretraining.py - model_fn_builder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model_fn_builder는 TPUEstimator를 사용하기 위해 closure를 리턴한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn_builder(bert_config, init_checkpoint, learning_rate,\n",
    "                     num_train_steps, num_warmup_steps, use_tpu,\n",
    "                     use_one_hot_embeddings):\n",
    "  \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n",
    "  \n",
    "  def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n",
    "  \"\"\"The `model_fn` for TPUEstimator.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "features의 값들을 상수에 할당한다. preprocess에서 파일에 썼던 값들을 다시 기록해보면,\n",
    "\n",
    "features 에 쓰인 값들은 아래와 같다.\n",
    "```\n",
    "input_ids = {list: 128} [2, 23, 11, 4, 68, 735, 3, 5, 4, 10213, 294, 735, 9, 577, 449, 4, 3, 0, 0, , ...]\n",
    "input_mask = {list: 128} [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, ...]\n",
    "segment_ids = {list: 128} [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, ...]\n",
    "masked_lm_positions = {list: 20} [3, 8, 15, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "masked_lm_ids = {list: 20} [8, 73, 287, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "masked_lm_ids = {list: 20} [8, 73, 287, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "masked_lm_weights = {list: 20} [1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...]\n",
    "next_sentence_label = {int} 0\n",
    "```\n",
    "\n",
    "각각의 값을 다시 정리해보면,\n",
    "```\n",
    "input_ids: token word의 id (int) 변환값\n",
    "input_mask: input_ids 길이만큼만 1, 나머지 0 (아마도 패딩 여부를 파악하기 위한 것으로 보인다)\n",
    "segment_ids: segment A는 0, B는 1\n",
    "masked_lm_positions: masked word의 위치 index\n",
    "masked_lm_ids: masked word의 id (int) 변환값\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "    input_ids = features[\"input_ids\"]\n",
    "    input_mask = features[\"input_mask\"]\n",
    "    segment_ids = features[\"segment_ids\"]\n",
    "    masked_lm_positions = features[\"masked_lm_positions\"]\n",
    "    masked_lm_ids = features[\"masked_lm_ids\"]\n",
    "    masked_lm_weights = features[\"masked_lm_weights\"]\n",
    "    next_sentence_labels = features[\"next_sentence_labels\"]\n",
    "\n",
    "    is_training = (mode == tf.estimator.ModeKeys.TRAIN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bert model을 정의해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = modeling.BertModel(\n",
    "        config=bert_config,\n",
    "        is_training=is_training,\n",
    "        input_ids=input_ids,\n",
    "        input_mask=input_mask,\n",
    "        token_type_ids=segment_ids,\n",
    "        use_one_hot_embeddings=use_one_hot_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "BertModel 의 생성자를 보면, transformer encoder 라고 되어 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class BertModel(object):\n",
    "  \"\"\"BERT model (\"Bidirectional Encoder Representations from Transformers\").\n",
    "\n",
    "  Example usage:\n",
    "\n",
    "  ```python\n",
    "  # Already been converted into WordPiece token ids\n",
    "  input_ids = tf.constant([[31, 51, 99], [15, 5, 0]])\n",
    "  input_mask = tf.constant([[1, 1, 1], [1, 1, 0]])\n",
    "  token_type_ids = tf.constant([[0, 0, 1], [0, 2, 0]])\n",
    "\n",
    "  config = modeling.BertConfig(vocab_size=32000, hidden_size=512,\n",
    "    num_hidden_layers=8, num_attention_heads=6, intermediate_size=1024)\n",
    "\n",
    "  model = modeling.BertModel(config=config, is_training=True,\n",
    "    input_ids=input_ids, input_mask=input_mask, token_type_ids=token_type_ids)\n",
    "\n",
    "  label_embeddings = tf.get_variable(...)\n",
    "  pooled_output = model.get_pooled_output()\n",
    "  logits = tf.matmul(pooled_output, label_embeddings)\n",
    "  ...\n",
    "  ```\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               config,\n",
    "               is_training,\n",
    "               input_ids,\n",
    "               input_mask=None,\n",
    "               token_type_ids=None,\n",
    "               use_one_hot_embeddings=False,\n",
    "               scope=None):\n",
    "    \"\"\"Constructor for BertModel.\n",
    "\n",
    "    Args:\n",
    "      config: `BertConfig` instance.\n",
    "      is_training: bool. true for training model, false for eval model. Controls\n",
    "        whether dropout will be applied.\n",
    "      input_ids: int32 Tensor of shape [batch_size, seq_length].\n",
    "      input_mask: (optional) int32 Tensor of shape [batch_size, seq_length].\n",
    "      token_type_ids: (optional) int32 Tensor of shape [batch_size, seq_length].\n",
    "      use_one_hot_embeddings: (optional) bool. Whether to use one-hot word\n",
    "        embeddings or tf.embedding_lookup() for the word embeddings.\n",
    "      scope: (optional) variable scope. Defaults to \"bert\".\n",
    "\n",
    "    Raises:\n",
    "      ValueError: The config is invalid or one of the input tensor shapes\n",
    "        is invalid.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "먼저 input의 shape를 구하고"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "    input_shape = get_shape_list(input_ids, expected_rank=2)\n",
    "    batch_size = input_shape[0]\n",
    "    seq_length = input_shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "embedding lookup을 수행하여 준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    with tf.variable_scope(scope, default_name=\"bert\"):\n",
    "      with tf.variable_scope(\"embeddings\"):\n",
    "        # Perform embedding lookup on the word ids.\n",
    "        (self.embedding_output, self.embedding_table) = embedding_lookup(\n",
    "            input_ids=input_ids,\n",
    "            vocab_size=config.vocab_size,\n",
    "            embedding_size=config.hidden_size,\n",
    "            initializer_range=config.initializer_range,\n",
    "            word_embedding_name=\"word_embeddings\",\n",
    "            use_one_hot_embeddings=use_one_hot_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_lookup(input_ids,\n",
    "                     vocab_size,\n",
    "                     embedding_size=128,\n",
    "                     initializer_range=0.02,\n",
    "                     word_embedding_name=\"word_embeddings\",\n",
    "                     use_one_hot_embeddings=False):\n",
    "  \"\"\"Looks up words embeddings for id tensor.\n",
    "\n",
    "  Args:\n",
    "    input_ids: int32 Tensor of shape [batch_size, seq_length] containing word\n",
    "      ids.\n",
    "    vocab_size: int. Size of the embedding vocabulary.\n",
    "    embedding_size: int. Width of the word embeddings.\n",
    "    initializer_range: float. Embedding initialization range.\n",
    "    word_embedding_name: string. Name of the embedding table.\n",
    "    use_one_hot_embeddings: bool. If True, use one-hot method for word\n",
    "      embeddings. If False, use `tf.gather()`.\n",
    "\n",
    "  Returns:\n",
    "    float Tensor of shape [batch_size, seq_length, embedding_size].\n",
    "  \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "먼저 input_ids가 2차원 (batch_size, seq_length) 이라면 차원을 하나 늘려준다.\n",
    "결과적으로 (batch_size, seq_length, 1) 이 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  # This function assumes that the input is of shape [batch_size, seq_length,\n",
    "  # num_inputs].\n",
    "  #\n",
    "  # If the input is a 2D tensor of shape [batch_size, seq_length], we\n",
    "  # reshape to [batch_size, seq_length, 1].\n",
    "  if input_ids.shape.ndims == 2:\n",
    "    input_ids = tf.expand_dims(input_ids, axis=[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hidden size에 맞는 embedding table (tf.Variable) 을 하나 만들어준다.\n",
    "embedding_size는 config에서 준 hidden_size (여기서는 768 이다.)\n",
    "즉 embedding_table은 (32000, 768) 이 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  embedding_table = tf.get_variable(\n",
    "      name=word_embedding_name,\n",
    "      shape=[vocab_size, embedding_size],\n",
    "      initializer=create_initializer(initializer_range))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input_ids를 flat 해서, embedding table에서 input_ids에 해당하는 vector를 가져온다.<br/>\n",
    "여기서 use_one_hot_embedding=True 이므로, input_ids를 곱해준다. <br/>\n",
    "- flat_input_ids = (8 * 128) = (1024,)\n",
    "- one_hot_input_ids = (1024, 32000)\n",
    "- output = (1024, 768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  flat_input_ids = tf.reshape(input_ids, [-1])\n",
    "  if use_one_hot_embeddings:\n",
    "    one_hot_input_ids = tf.one_hot(flat_input_ids, depth=vocab_size)\n",
    "    output = tf.matmul(one_hot_input_ids, embedding_table)\n",
    "  else:\n",
    "    output = tf.gather(embedding_table, flat_input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그 후 output을 input_ids의 shape에 맞게 복원해준다. 이 때 마지막 차원에 embedding_size를 곱해준다.\n",
    "- input_shape = [8, 128, 1]\n",
    "- output = (8, 128, 768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  input_shape = get_shape_list(input_ids)\n",
    "\n",
    "  output = tf.reshape(output,\n",
    "                      input_shape[0:-1] + [input_shape[-1] * embedding_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "embedding vector들에 positional embedding, token type embedding을 더해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        # Add positional embeddings and token type embeddings, then layer\n",
    "        # normalize and perform dropout.\n",
    "        self.embedding_output = embedding_postprocessor(\n",
    "            input_tensor=self.embedding_output,\n",
    "            use_token_type=True,\n",
    "            token_type_ids=token_type_ids,\n",
    "            token_type_vocab_size=config.type_vocab_size,\n",
    "            token_type_embedding_name=\"token_type_embeddings\",\n",
    "            use_position_embeddings=True,\n",
    "            position_embedding_name=\"position_embeddings\",\n",
    "            initializer_range=config.initializer_range,\n",
    "            max_position_embeddings=config.max_position_embeddings,\n",
    "            dropout_prob=config.hidden_dropout_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_postprocessor(input_tensor,\n",
    "                            use_token_type=False,\n",
    "                            token_type_ids=None,\n",
    "                            token_type_vocab_size=16,\n",
    "                            token_type_embedding_name=\"token_type_embeddings\",\n",
    "                            use_position_embeddings=True,\n",
    "                            position_embedding_name=\"position_embeddings\",\n",
    "                            initializer_range=0.02,\n",
    "                            max_position_embeddings=512,\n",
    "                            dropout_prob=0.1):\n",
    "  \"\"\"Performs various post-processing on a word embedding tensor.\n",
    "\n",
    "  Args:\n",
    "    input_tensor: float Tensor of shape [batch_size, seq_length,\n",
    "      embedding_size].\n",
    "    use_token_type: bool. Whether to add embeddings for `token_type_ids`.\n",
    "    token_type_ids: (optional) int32 Tensor of shape [batch_size, seq_length].\n",
    "      Must be specified if `use_token_type` is True.\n",
    "    token_type_vocab_size: int. The vocabulary size of `token_type_ids`.\n",
    "    token_type_embedding_name: string. The name of the embedding table variable\n",
    "      for token type ids.\n",
    "    use_position_embeddings: bool. Whether to add position embeddings for the\n",
    "      position of each token in the sequence.\n",
    "    position_embedding_name: string. The name of the embedding table variable\n",
    "      for positional embeddings.\n",
    "    initializer_range: float. Range of the weight initialization.\n",
    "    max_position_embeddings: int. Maximum sequence length that might ever be\n",
    "      used with this model. This can be longer than the sequence length of\n",
    "      input_tensor, but cannot be shorter.\n",
    "    dropout_prob: float. Dropout probability applied to the final output tensor.\n",
    "\n",
    "  Returns:\n",
    "    float tensor with same shape as `input_tensor`.\n",
    "\n",
    "  Raises:\n",
    "    ValueError: One of the tensor shapes or input values is invalid.\n",
    "  \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "먼저 parameter를 살펴보면 아래와 같다.\n",
    "\n",
    "```\n",
    "input_tensor=Tensor(8, 128, 768)\n",
    "use_token_type=True\n",
    "token_type_ids=Tensor(8, 128)\n",
    "token_type_vocab_size=2\n",
    "token_type_embedding_name='token_type_embeddings'\n",
    "use_position_embeddings=True,\n",
    "position_embedding_name=\"position_embeddings\",\n",
    "initializer_range=0.02,\n",
    "max_position_embeddings=512,\n",
    "dropout_prob=0.1\n",
    "```\n",
    "\n",
    "input_tensor의 shape를 변수에 할당한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  input_shape = get_shape_list(input_tensor, expected_rank=3)\n",
    "  batch_size = input_shape[0]\n",
    "  seq_length = input_shape[1]\n",
    "  width = input_shape[2]\n",
    "\n",
    "  output = input_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "token type을 embedding vector (one hot) 형식으로 변환하여 output 에 더해준다.\n",
    "- token_type_table=tf.Variable(2, 768) \n",
    "- flat_token_type_ids=(1024,)\n",
    "- one_hot_ids=(1024, 2)\n",
    "- token_type_embeddings=(1024, 768) -> (8, 128, 768)\n",
    "- output=(8, 128, 768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  if use_token_type:\n",
    "    if token_type_ids is None:\n",
    "      raise ValueError(\"`token_type_ids` must be specified if\"\n",
    "                       \"`use_token_type` is True.\")\n",
    "    token_type_table = tf.get_variable(\n",
    "        name=token_type_embedding_name,\n",
    "        shape=[token_type_vocab_size, width],\n",
    "        initializer=create_initializer(initializer_range))\n",
    "    # This vocab will be small so we always do one-hot here, since it is always\n",
    "    # faster for a small vocabulary.\n",
    "    flat_token_type_ids = tf.reshape(token_type_ids, [-1])\n",
    "    one_hot_ids = tf.one_hot(flat_token_type_ids, depth=token_type_vocab_size)\n",
    "    token_type_embeddings = tf.matmul(one_hot_ids, token_type_table)\n",
    "    token_type_embeddings = tf.reshape(token_type_embeddings,\n",
    "                                       [batch_size, seq_length, width])\n",
    "    output += token_type_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그 후 positional embedding도 output에 더해준다.<br/>\n",
    "position embeddings를 output에 더할 때는 <br/>\n",
    "마지막 2개의 차원만 seq length, width와 관련이 있으므로 (1, seq length, width)로 만들어 output 에 더해준다.<br/>\n",
    "output의 차원이 (batch size, seq length, width) 이므로, position embedding은 tf의 broadcast 기능으로 모든 batch에 더해진다.\n",
    "\n",
    "\n",
    "```\n",
    "full_position_embeddings=tf.Variable(512, 768)\n",
    "position_embeddings=(128, 768) -> (1, 128, 768)\n",
    "num_dims=3\n",
    "position_broadcast_shape=[1] -> [1, 128, 768]\n",
    "position_embeddings=(1, 128, 768)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  if use_position_embeddings:\n",
    "    assert_op = tf.assert_less_equal(seq_length, max_position_embeddings)\n",
    "    with tf.control_dependencies([assert_op]):\n",
    "      full_position_embeddings = tf.get_variable(\n",
    "          name=position_embedding_name,\n",
    "          shape=[max_position_embeddings, width],\n",
    "          initializer=create_initializer(initializer_range))\n",
    "      # Since the position embedding table is a learned variable, we create it\n",
    "      # using a (long) sequence length `max_position_embeddings`. The actual\n",
    "      # sequence length might be shorter than this, for faster training of\n",
    "      # tasks that do not have long sequences.\n",
    "      #\n",
    "      # So `full_position_embeddings` is effectively an embedding table\n",
    "      # for position [0, 1, 2, ..., max_position_embeddings-1], and the current\n",
    "      # sequence has positions [0, 1, 2, ... seq_length-1], so we can just\n",
    "      # perform a slice.\n",
    "      position_embeddings = tf.slice(full_position_embeddings, [0, 0],\n",
    "                                     [seq_length, -1])\n",
    "      num_dims = len(output.shape.as_list())\n",
    "\n",
    "      # Only the last two dimensions are relevant (`seq_length` and `width`), so\n",
    "      # we broadcast among the first dimensions, which is typically just\n",
    "      # the batch size.\n",
    "      position_broadcast_shape = []\n",
    "      for _ in range(num_dims - 2):\n",
    "        position_broadcast_shape.append(1)\n",
    "      position_broadcast_shape.extend([seq_length, width])\n",
    "      position_embeddings = tf.reshape(position_embeddings,\n",
    "                                       position_broadcast_shape)\n",
    "      output += position_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "post process의 마지막으로 output에 layer normalization & dropout을 수행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  output = layer_norm_and_dropout(output, dropout_prob)\n",
    "  \n",
    "  return output\n",
    "\n",
    "\n",
    "def layer_norm_and_dropout(input_tensor, dropout_prob, name=None):\n",
    "  \"\"\"Runs layer normalization followed by dropout.\"\"\"\n",
    "  output_tensor = layer_norm(input_tensor, name)\n",
    "  output_tensor = dropout(output_tensor, dropout_prob)\n",
    "  return output_tensor\n",
    "\n",
    "def layer_norm(input_tensor, name=None):\n",
    "  \"\"\"Run layer normalization on the last dimension of the tensor.\"\"\"\n",
    "  return tf.contrib.layers.layer_norm(\n",
    "      inputs=input_tensor, begin_norm_axis=-1, begin_params_axis=-1, scope=name)\n",
    "\n",
    "def dropout(input_tensor, dropout_prob):\n",
    "  \"\"\"Perform dropout.\n",
    "\n",
    "  Args:\n",
    "    input_tensor: float Tensor.\n",
    "    dropout_prob: Python float. The probability of dropping out a value (NOT of\n",
    "      *keeping* a dimension as in `tf.nn.dropout`).\n",
    "\n",
    "  Returns:\n",
    "    A version of `input_tensor` with dropout applied.\n",
    "  \"\"\"\n",
    "  if dropout_prob is None or dropout_prob == 0.0:\n",
    "    return input_tensor\n",
    "\n",
    "  output = tf.nn.dropout(input_tensor, 1.0 - dropout_prob)\n",
    "  return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BertModel의 생성자로 돌아와서, self.embedding_output에 embedding lookup & postprocessing한 결과를 할당했다.\n",
    "- self.embedding_output=Tensor(8, 128, 768)\n",
    "- self.embedding_table=tf.Variable(32000, 768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(self.embedding_output, self.embedding_table) = embedding_lookup(...\n",
    "self.embedding_output = embedding_postprocessor(..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "attention을 적용하기 위해서 [batch size, seq length] 인 input을 [batch size, seq length, seq length] 로 바꿔준다.<br/>\n",
    "remind를 해보면,\n",
    "- input_ids: 단어의 id index 값들\n",
    "- input_mask: input_ids 의 길이 만큼 1, 나머지 (padding) 는 0으로 둔 masking\n",
    "이다.<br/>\n",
    "<br/>\n",
    "함수 안의 변수들을 보면,\n",
    "- batch_size=8\n",
    "- from_seq_length=128\n",
    "- to_shape=[8, 128]\n",
    "- to_seq_length=128\n",
    "- to_mask=Tensor(8, 1, 128)\n",
    "- broadcast_ones=(8, 128, 1)\n",
    "- mask=(8, 128, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "      with tf.variable_scope(\"encoder\"):\n",
    "        # This converts a 2D mask of shape [batch_size, seq_length] to a 3D\n",
    "        # mask of shape [batch_size, seq_length, seq_length] which is used\n",
    "        # for the attention scores.\n",
    "        attention_mask = create_attention_mask_from_input_mask(\n",
    "            input_ids, input_mask)\n",
    "        \n",
    "        \n",
    "def create_attention_mask_from_input_mask(from_tensor, to_mask):\n",
    "  \"\"\"Create 3D attention mask from a 2D tensor mask.\n",
    "\n",
    "  Args:\n",
    "    from_tensor: 2D or 3D Tensor of shape [batch_size, from_seq_length, ...].\n",
    "    to_mask: int32 Tensor of shape [batch_size, to_seq_length].\n",
    "\n",
    "  Returns:\n",
    "    float Tensor of shape [batch_size, from_seq_length, to_seq_length].\n",
    "  \"\"\"\n",
    "  from_shape = get_shape_list(from_tensor, expected_rank=[2, 3])\n",
    "  batch_size = from_shape[0]\n",
    "  from_seq_length = from_shape[1]\n",
    "\n",
    "  to_shape = get_shape_list(to_mask, expected_rank=2)\n",
    "  to_seq_length = to_shape[1]\n",
    "\n",
    "  to_mask = tf.cast(\n",
    "      tf.reshape(to_mask, [batch_size, 1, to_seq_length]), tf.float32)\n",
    "\n",
    "  # We don't assume that `from_tensor` is a mask (although it could be). We\n",
    "  # don't actually care if we attend *from* padding tokens (only *to* padding)\n",
    "  # tokens so we create a tensor of all ones.\n",
    "  #\n",
    "  # `broadcast_ones` = [batch_size, from_seq_length, 1]\n",
    "  broadcast_ones = tf.ones(\n",
    "      shape=[batch_size, from_seq_length, 1], dtype=tf.float32)\n",
    "\n",
    "  # Here we broadcast along two dimensions to create the mask.\n",
    "  mask = broadcast_ones * to_mask\n",
    "\n",
    "  return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mask가 어떻게 계산될 것인지 좀 더 살펴보자.\n",
    "- mask = broadcast_ones * to_mask = (8, 128, 1) * (8, 1, 128)\n",
    "\n",
    "결과적으로 mask = Tensor(8, 128, 128)이 된다. <br/>\n",
    "즉, input_mask 를 2D로 broadcast 한 matrix가 만들어진다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t2 shape: (2, 3)\n",
      "t2 shape: (2, 3, 1)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "t1 = tf.ones(shape=(2, 1, 3), dtype=tf.int64)\n",
    "#t2 = tf.random.uniform(shape=(2, 3, 1), minval=0, maxval=10, dtype=tf.int32)\n",
    "t2 = tf.convert_to_tensor(np.array([[1, 1, 0], [1, 1, 0]]))\n",
    "print('t2 shape:', t2.shape)\n",
    "t2 = tf.reshape(t2, [2, 3, 1])\n",
    "print('t2 shape:', t2.shape)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1, 1, 1]],\n",
       "\n",
       "       [[1, 1, 1]]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1],\n",
       "        [1],\n",
       "        [0]],\n",
       "\n",
       "       [[1],\n",
       "        [1],\n",
       "        [0]]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       [[1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [0, 0, 0]]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(t1 * t2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create_attention_mask_from_input_mask에서 to_mask는 input_mask이다.<br/>\n",
    "input_mask는 input_ids의 길이만큼 1이 들어 있으므로, input_mask를 길이만큼 복사해서 채워넣게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "다시 BertModel의 생성자로 돌아와서, 아래 attention_mask는 (batch size, seq_length, seq_length) -> (8, 128, 128) 이 된다.\n",
    "\n",
    "```\n",
    "attention_mask = create_attention_mask_from_input_mask(\n",
    "            input_ids, input_mask)\n",
    "```\n",
    "\n",
    "이 mask로 encoder layer를 구성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        # Run the stacked transformer.\n",
    "        # `sequence_output` shape = [batch_size, seq_length, hidden_size].\n",
    "        self.all_encoder_layers = transformer_model(\n",
    "            input_tensor=self.embedding_output,\n",
    "            attention_mask=attention_mask,\n",
    "            hidden_size=config.hidden_size,\n",
    "            num_hidden_layers=config.num_hidden_layers,\n",
    "            num_attention_heads=config.num_attention_heads,\n",
    "            intermediate_size=config.intermediate_size,\n",
    "            intermediate_act_fn=get_activation(config.hidden_act),\n",
    "            hidden_dropout_prob=config.hidden_dropout_prob,\n",
    "            attention_probs_dropout_prob=config.attention_probs_dropout_prob,\n",
    "            initializer_range=config.initializer_range,\n",
    "            do_return_all_layers=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "function parameter를 보면,\n",
    "```\n",
    "input_tensor=Tensor(8, 128, 768)\n",
    "attention_mask=Tensor(8, 128, 128)\n",
    "hidden_size=768,\n",
    "num_hidden_layers=12,\n",
    "num_attention_heads=12,\n",
    "intermediate_size=3072,\n",
    "intermediate_act_fn=gelu,\n",
    "hidden_dropout_prob=0.1,\n",
    "attention_probs_dropout_prob=0.1,\n",
    "initializer_range=0.02,\n",
    "do_return_all_layers=True\n",
    "```\n",
    "\n",
    "먼저 input shape를 변수에 할당한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_model(input_tensor,\n",
    "                      attention_mask=None,\n",
    "                      hidden_size=768,\n",
    "                      num_hidden_layers=12,\n",
    "                      num_attention_heads=12,\n",
    "                      intermediate_size=3072,\n",
    "                      intermediate_act_fn=gelu,\n",
    "                      hidden_dropout_prob=0.1,\n",
    "                      attention_probs_dropout_prob=0.1,\n",
    "                      initializer_range=0.02,\n",
    "                      do_return_all_layers=False):\n",
    "  \"\"\"Multi-headed, multi-layer Transformer from \"Attention is All You Need\".\n",
    "\n",
    "  This is almost an exact implementation of the original Transformer encoder.\n",
    "\n",
    "  See the original paper:\n",
    "  https://arxiv.org/abs/1706.03762\n",
    "\n",
    "  Also see:\n",
    "  https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/transformer.py\n",
    "\n",
    "  Args:\n",
    "    input_tensor: float Tensor of shape [batch_size, seq_length, hidden_size].\n",
    "    attention_mask: (optional) int32 Tensor of shape [batch_size, seq_length,\n",
    "      seq_length], with 1 for positions that can be attended to and 0 in\n",
    "      positions that should not be.\n",
    "    hidden_size: int. Hidden size of the Transformer.\n",
    "    num_hidden_layers: int. Number of layers (blocks) in the Transformer.\n",
    "    num_attention_heads: int. Number of attention heads in the Transformer.\n",
    "    intermediate_size: int. The size of the \"intermediate\" (a.k.a., feed\n",
    "      forward) layer.\n",
    "    intermediate_act_fn: function. The non-linear activation function to apply\n",
    "      to the output of the intermediate/feed-forward layer.\n",
    "    hidden_dropout_prob: float. Dropout probability for the hidden layers.\n",
    "    attention_probs_dropout_prob: float. Dropout probability of the attention\n",
    "      probabilities.\n",
    "    initializer_range: float. Range of the initializer (stddev of truncated\n",
    "      normal).\n",
    "    do_return_all_layers: Whether to also return all layers or just the final\n",
    "      layer.\n",
    "\n",
    "  Returns:\n",
    "    float Tensor of shape [batch_size, seq_length, hidden_size], the final\n",
    "    hidden layer of the Transformer.\n",
    "\n",
    "  Raises:\n",
    "    ValueError: A Tensor shape or parameter is invalid.\n",
    "  \"\"\"\n",
    "  if hidden_size % num_attention_heads != 0:\n",
    "    raise ValueError(\n",
    "        \"The hidden size (%d) is not a multiple of the number of attention \"\n",
    "        \"heads (%d)\" % (hidden_size, num_attention_heads))\n",
    "\n",
    "  attention_head_size = int(hidden_size / num_attention_heads)\n",
    "  input_shape = get_shape_list(input_tensor, expected_rank=3)\n",
    "  batch_size = input_shape[0]\n",
    "  seq_length = input_shape[1]\n",
    "  input_width = input_shape[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input tensor를 reshape한다. <br/>\n",
    "그리고 2D <--> 3D로 변경하는 작업이 CPU, GPU에서는 부하가 없으나 TPU에서는 부하가 있을 수도 있기 때문에 2D로 계속 유지한다.\n",
    "(batch size, seq length, width) --> (batch size * seq length, width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  # We keep the representation as a 2D tensor to avoid re-shaping it back and\n",
    "  # forth from a 3D tensor to a 2D tensor. Re-shapes are normally free on\n",
    "  # the GPU/CPU but may not be free on the TPU, so we want to minimize them to\n",
    "  # help the optimizer.\n",
    "  prev_output = reshape_to_matrix(input_tensor)\n",
    "  \n",
    "def reshape_to_matrix(input_tensor):\n",
    "  \"\"\"Reshapes a >= rank 2 tensor to a rank 2 tensor (i.e., a matrix).\"\"\"\n",
    "  ndims = input_tensor.shape.ndims\n",
    "  if ndims < 2:\n",
    "    raise ValueError(\"Input tensor must have at least rank 2. Shape = %s\" %\n",
    "                     (input_tensor.shape))\n",
    "  if ndims == 2:\n",
    "    return input_tensor\n",
    "\n",
    "  width = input_tensor.shape[-1]\n",
    "  output_tensor = tf.reshape(input_tensor, [-1, width])\n",
    "  return output_tensor  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "num_hidden_layer만큼 attention layer를 생성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  all_layer_outputs = []\n",
    "  for layer_idx in range(num_hidden_layers):\n",
    "    with tf.variable_scope(\"layer_%d\" % layer_idx):\n",
    "      layer_input = prev_output\n",
    "\n",
    "      with tf.variable_scope(\"attention\"):\n",
    "        attention_heads = []\n",
    "        with tf.variable_scope(\"self\"):\n",
    "          attention_head = attention_layer(\n",
    "              from_tensor=layer_input,\n",
    "              to_tensor=layer_input,\n",
    "              attention_mask=attention_mask,\n",
    "              num_attention_heads=num_attention_heads,\n",
    "              size_per_head=attention_head_size,\n",
    "              attention_probs_dropout_prob=attention_probs_dropout_prob,\n",
    "              initializer_range=initializer_range,\n",
    "              do_return_2d_tensor=True,\n",
    "              batch_size=batch_size,\n",
    "              from_seq_length=seq_length,\n",
    "              to_seq_length=seq_length)\n",
    "          attention_heads.append(attention_head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래는 attention_layer 도입부로, attention 에 대한 설명과 parameter들에 대한 설명이 되어 있다. <br/>\n",
    "attention layer의 수식은 다음과 같다.\n",
    "\\begin{align}\n",
    "{ Attention } ( Q , K , V ) = \\operatorname { softmax } \\left( \\frac { Q K ^ { T } } { \\sqrt { d _ { k } } } \\right) V\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "단 여기서는 multi-head attention을 사용한다.\n",
    "\\begin{aligned}\n",
    "\\text{MultiHead}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) &= [\\text{head}_1; \\dots; \\text{head}_h]\\mathbf{W}^O \\\\\n",
    "\\text{where head}_i &= \\text{Attention}(\\mathbf{Q}\\mathbf{W}^Q_i, \\mathbf{K}\\mathbf{W}^K_i, \\mathbf{V}\\mathbf{W}^V_i)\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_layer(from_tensor,\n",
    "                    to_tensor,\n",
    "                    attention_mask=None,\n",
    "                    num_attention_heads=1,\n",
    "                    size_per_head=512,\n",
    "                    query_act=None,\n",
    "                    key_act=None,\n",
    "                    value_act=None,\n",
    "                    attention_probs_dropout_prob=0.0,\n",
    "                    initializer_range=0.02,\n",
    "                    do_return_2d_tensor=False,\n",
    "                    batch_size=None,\n",
    "                    from_seq_length=None,\n",
    "                    to_seq_length=None):\n",
    "  \"\"\"Performs multi-headed attention from `from_tensor` to `to_tensor`.\n",
    "\n",
    "  This is an implementation of multi-headed attention based on \"Attention\n",
    "  is all you Need\". If `from_tensor` and `to_tensor` are the same, then\n",
    "  this is self-attention. Each timestep in `from_tensor` attends to the\n",
    "  corresponding sequence in `to_tensor`, and returns a fixed-with vector.\n",
    "\n",
    "  This function first projects `from_tensor` into a \"query\" tensor and\n",
    "  `to_tensor` into \"key\" and \"value\" tensors. These are (effectively) a list\n",
    "  of tensors of length `num_attention_heads`, where each tensor is of shape\n",
    "  [batch_size, seq_length, size_per_head].\n",
    "\n",
    "  Then, the query and key tensors are dot-producted and scaled. These are\n",
    "  softmaxed to obtain attention probabilities. The value tensors are then\n",
    "  interpolated by these probabilities, then concatenated back to a single\n",
    "  tensor and returned.\n",
    "\n",
    "  In practice, the multi-headed attention are done with transposes and\n",
    "  reshapes rather than actual separate tensors.\n",
    "\n",
    "  Args:\n",
    "    from_tensor: float Tensor of shape [batch_size, from_seq_length,\n",
    "      from_width].\n",
    "    to_tensor: float Tensor of shape [batch_size, to_seq_length, to_width].\n",
    "    attention_mask: (optional) int32 Tensor of shape [batch_size,\n",
    "      from_seq_length, to_seq_length]. The values should be 1 or 0. The\n",
    "      attention scores will effectively be set to -infinity for any positions in\n",
    "      the mask that are 0, and will be unchanged for positions that are 1.\n",
    "    num_attention_heads: int. Number of attention heads.\n",
    "    size_per_head: int. Size of each attention head.\n",
    "    query_act: (optional) Activation function for the query transform.\n",
    "    key_act: (optional) Activation function for the key transform.\n",
    "    value_act: (optional) Activation function for the value transform.\n",
    "    attention_probs_dropout_prob: (optional) float. Dropout probability of the\n",
    "      attention probabilities.\n",
    "    initializer_range: float. Range of the weight initializer.\n",
    "    do_return_2d_tensor: bool. If True, the output will be of shape [batch_size\n",
    "      * from_seq_length, num_attention_heads * size_per_head]. If False, the\n",
    "      output will be of shape [batch_size, from_seq_length, num_attention_heads\n",
    "      * size_per_head].\n",
    "    batch_size: (Optional) int. If the input is 2D, this might be the batch size\n",
    "      of the 3D version of the `from_tensor` and `to_tensor`.\n",
    "    from_seq_length: (Optional) If the input is 2D, this might be the seq length\n",
    "      of the 3D version of the `from_tensor`.\n",
    "    to_seq_length: (Optional) If the input is 2D, this might be the seq length\n",
    "      of the 3D version of the `to_tensor`.\n",
    "\n",
    "  Returns:\n",
    "    float Tensor of shape [batch_size, from_seq_length,\n",
    "      num_attention_heads * size_per_head]. (If `do_return_2d_tensor` is\n",
    "      true, this will be of shape [batch_size * from_seq_length,\n",
    "      num_attention_heads * size_per_head]).\n",
    "\n",
    "  Raises:\n",
    "    ValueError: Any of the arguments or tensor shapes are invalid.\n",
    "  \"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "먼저 input, output의 shape를 변수에 할당한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  from_shape = get_shape_list(from_tensor, expected_rank=[2, 3])\n",
    "  to_shape = get_shape_list(to_tensor, expected_rank=[2, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그리고 batch size, from과 to의 seq length를 변수로 할당한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  if len(from_shape) == 3:\n",
    "    batch_size = from_shape[0]\n",
    "    from_seq_length = from_shape[1]\n",
    "    to_seq_length = to_shape[1]\n",
    "  elif len(from_shape) == 2:\n",
    "    if (batch_size is None or from_seq_length is None or to_seq_length is None):\n",
    "      raise ValueError(\n",
    "          \"When passing in rank 2 tensors to attention_layer, the values \"\n",
    "          \"for `batch_size`, `from_seq_length`, and `to_seq_length` \"\n",
    "          \"must all be specified.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from, to를 2D로 변환해둔다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  # Scalar dimensions referenced here:\n",
    "  #   B = batch size (number of sequences)\n",
    "  #   F = `from_tensor` sequence length\n",
    "  #   T = `to_tensor` sequence length\n",
    "  #   N = `num_attention_heads`\n",
    "  #   H = `size_per_head`\n",
    "\n",
    "  from_tensor_2d = reshape_to_matrix(from_tensor)\n",
    "  to_tensor_2d = reshape_to_matrix(to_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "query, key, value layer를 정의한다.\n",
    "- from_tensor_2d = Tensor(1024, 768)\n",
    "- to_tensor_2d = Tensor(1024, 768)\n",
    "- num_attention_heads = 12\n",
    "- size_per_head = 64\n",
    "- num_attention_heads * size_per_head = 768\n",
    "- query_act, key_act, value_act = None\n",
    "- initializer_range = 0.02\n",
    "\n",
    "query, key, value layer는 모두 Tensor(1024, 768) 이다.<br/>\n",
    "즉, 여기서는 from과 to가 같으므로 모두 (B * F, N * H) 이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  # `query_layer` = [B*F, N*H]\n",
    "  query_layer = tf.layers.dense(\n",
    "      from_tensor_2d,\n",
    "      num_attention_heads * size_per_head,\n",
    "      activation=query_act,\n",
    "      name=\"query\",\n",
    "      kernel_initializer=create_initializer(initializer_range))\n",
    "\n",
    "  # `key_layer` = [B*T, N*H]\n",
    "  key_layer = tf.layers.dense(\n",
    "      to_tensor_2d,\n",
    "      num_attention_heads * size_per_head,\n",
    "      activation=key_act,\n",
    "      name=\"key\",\n",
    "      kernel_initializer=create_initializer(initializer_range))\n",
    "\n",
    "  # `value_layer` = [B*T, N*H]\n",
    "  value_layer = tf.layers.dense(\n",
    "      to_tensor_2d,\n",
    "      num_attention_heads * size_per_head,\n",
    "      activation=value_act,\n",
    "      name=\"value\",\n",
    "      kernel_initializer=create_initializer(initializer_range))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "query, key layer를 transpose 한다. <br/>\n",
    "(B * F, N * H) -> (B, N, F, H)로 변경한다. <br/>\n",
    "여기서는 (8, 12, 128, 64) 이다. <br/>\n",
    "<br/>\n",
    "즉, multiple head attention을 적용하기 위해서 batch size * seq length (8 * 128 = 1024) 로 flat 되어 있었던 Tensor를 (batch size, seq length)로 쪼개고, <br/>\n",
    "N * H (12 * 64 = 768) 을 나누어서 (B, F, N, H) 로 만든 다음, <br/>\n",
    "index를 (0, 1, 2, 3) -> (0, 2, 1, 3) 으로 변경시켜서 N (num_attention_heads)과 F (seq length)의 위치를 바꿔준다. <br/>\n",
    "이렇게 함으로써 Dot product를 수행할 때 (seq length, head size) * (seq length, head size) 로 곱해질 수 있도록 만든다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  def transpose_for_scores(input_tensor, batch_size, num_attention_heads,\n",
    "                           seq_length, width):\n",
    "    output_tensor = tf.reshape(\n",
    "        input_tensor, [batch_size, seq_length, num_attention_heads, width])\n",
    "\n",
    "    output_tensor = tf.transpose(output_tensor, [0, 2, 1, 3])\n",
    "    return output_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  # `query_layer` = [B, N, F, H]\n",
    "  query_layer = transpose_for_scores(query_layer, batch_size,\n",
    "                                     num_attention_heads, from_seq_length,\n",
    "                                     size_per_head)\n",
    "\n",
    "  # `key_layer` = [B, N, T, H]\n",
    "  key_layer = transpose_for_scores(key_layer, batch_size, num_attention_heads,\n",
    "                                   to_seq_length, size_per_head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "query와 key를 곱해 attention score를 구한다.<br/>\n",
    "그 후 attention head size의 제곱근으로 나누어 준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  # Take the dot product between \"query\" and \"key\" to get the raw\n",
    "  # attention scores.\n",
    "  # `attention_scores` = [B, N, F, T]\n",
    "  attention_scores = tf.matmul(query_layer, key_layer, transpose_b=True)\n",
    "  attention_scores = tf.multiply(attention_scores,\n",
    "                                 1.0 / math.sqrt(float(size_per_head)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input mask에서 0으로 표시된 값을 -INFINITY (또는 매우 작은 음수. 여기서는 -10000) 로 만들어 attention score에 더해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  if attention_mask is not None:\n",
    "    # `attention_mask` = [B, 1, F, T]\n",
    "    attention_mask = tf.expand_dims(attention_mask, axis=[1])\n",
    "\n",
    "    # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n",
    "    # masked positions, this operation will create a tensor which is 0.0 for\n",
    "    # positions we want to attend and -10000.0 for masked positions.\n",
    "    adder = (1.0 - tf.cast(attention_mask, tf.float32)) * -10000.0\n",
    "\n",
    "    # Since we are adding it to the raw scores before the softmax, this is\n",
    "    # effectively the same as removing these entirely.\n",
    "    attention_scores += adder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "attention score에 softmax를 취해서 확률값으로 변경한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the attention scores to probabilities.\n",
    "  # `attention_probs` = [B, N, F, T]\n",
    "  attention_probs = tf.nn.softmax(attention_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그 값에 dropout을 취해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  # This is actually dropping out entire tokens to attend to, which might\n",
    "  # seem a bit unusual, but is taken from the original Transformer paper.\n",
    "  attention_probs = dropout(attention_probs, attention_probs_dropout_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "value layer (B * T, N * H) = (1024, 768)을 attention_probs (B, N, F, T) = (8, 12, 128, 128)을 곱할 수 있는 형태로 바꿔준다. <br/>\n",
    "결과적으로 value layer (B, N, T, H) = (8, 12, 128, 64) 가 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  # `value_layer` = [B, T, N, H]\n",
    "  value_layer = tf.reshape(\n",
    "      value_layer,\n",
    "      [batch_size, to_seq_length, num_attention_heads, size_per_head])\n",
    "\n",
    "  # `value_layer` = [B, N, T, H]\n",
    "  value_layer = tf.transpose(value_layer, [0, 2, 1, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "value를 attention probs에 곱해준 후 원래 형태로 되돌린다.<br/>\n",
    "context layer는 (B, F, N, H) = (8, 128, 12, 64) 가 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  # `context_layer` = [B, N, F, H]\n",
    "  context_layer = tf.matmul(attention_probs, value_layer)\n",
    "\n",
    "  # `context_layer` = [B, F, N, H]\n",
    "  context_layer = tf.transpose(context_layer, [0, 2, 1, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "나눴던 head를 합친다. 이 때 2D tensor가 필요하면 B, F도 합쳐서 하나로 만든다.\n",
    "여기서는 2D tensor가 필요하므로 최종적으로 context layer는 (B * F, N * H) = (1024, 768) 이 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  if do_return_2d_tensor:\n",
    "    # `context_layer` = [B*F, N*H]\n",
    "    context_layer = tf.reshape(\n",
    "        context_layer,\n",
    "        [batch_size * from_seq_length, num_attention_heads * size_per_head])\n",
    "  else:\n",
    "    # `context_layer` = [B, F, N*H]\n",
    "    context_layer = tf.reshape(\n",
    "        context_layer,\n",
    "        [batch_size, from_seq_length, num_attention_heads * size_per_head])\n",
    "\n",
    "  return context_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "transformer_model 함수로 되돌아와서, 만들어진 attention head를 리스트에 담는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_head = attention_layer(\n",
    "              from_tensor=layer_input,\n",
    "              to_tensor=layer_input,\n",
    "              attention_mask=attention_mask,\n",
    "              num_attention_heads=num_attention_heads,\n",
    "              size_per_head=attention_head_size,\n",
    "              attention_probs_dropout_prob=attention_probs_dropout_prob,\n",
    "              initializer_range=initializer_range,\n",
    "              do_return_2d_tensor=True,\n",
    "              batch_size=batch_size,\n",
    "              from_seq_length=seq_length,\n",
    "              to_seq_length=seq_length)\n",
    "attention_heads.append(attention_head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그리고 전체를 concat으로 이어준다. 여기서는 이미 head를 하나로 합쳤으므로, attention_head[0]이 attention_ouput이 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        attention_output = None\n",
    "        if len(attention_heads) == 1:\n",
    "          attention_output = attention_heads[0]\n",
    "        else:\n",
    "          # In the case where we have other sequences, we just concatenate\n",
    "          # them to the self-attention head before the projection.\n",
    "          attention_output = tf.concat(attention_heads, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그 attention output 전체에 hidden size (768) 만큼의 dense weight를 곱해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "          attention_output = tf.layers.dense(\n",
    "              attention_output,\n",
    "              hidden_size,\n",
    "              kernel_initializer=create_initializer(initializer_range))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "attention output에 dropout을 취한 뒤, 이전 layer의 output (현재 layer의 input) 과 합쳐서 layer norm 을 수행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "          attention_output = dropout(attention_output, hidden_dropout_prob)\n",
    "          attention_output = layer_norm(attention_output + layer_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dense를 취해 중간 layer를 만들고"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "      # The activation is only applied to the \"intermediate\" hidden layer.\n",
    "      with tf.variable_scope(\"intermediate\"):\n",
    "        intermediate_output = tf.layers.dense(\n",
    "            attention_output,\n",
    "            intermediate_size,\n",
    "            activation=intermediate_act_fn,\n",
    "            kernel_initializer=create_initializer(initializer_range))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "output이 input과 크기가 같아지도록 다시 한번 dense를 취해준다.<br/>\n",
    "이 마지막 output은 (B * F, N * H) = (1024, 768) 이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "      # Down-project back to `hidden_size` then add the residual.\n",
    "      with tf.variable_scope(\"output\"):\n",
    "        layer_output = tf.layers.dense(\n",
    "            intermediate_output,\n",
    "            hidden_size,\n",
    "            kernel_initializer=create_initializer(initializer_range))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "마지막으로 dropout, layer norm을 취해주고"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        layer_output = dropout(layer_output, hidden_dropout_prob)\n",
    "        layer_output = layer_norm(layer_output + attention_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음 layer를 위해 prev_output에 현재 output을 저장한 뒤 all_layer_output에 현재 output을 넣는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        prev_output = layer_output\n",
    "        all_layer_outputs.append(layer_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모든 결과를 리턴할지, 마지막 값만 리턴할지 설정에 따라 리턴한다.<br/>\n",
    "여기서 reshape 결과는 (8, 128, 768) 이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  if do_return_all_layers:\n",
    "    final_outputs = []\n",
    "    for layer_output in all_layer_outputs:\n",
    "      final_output = reshape_from_matrix(layer_output, input_shape)\n",
    "      final_outputs.append(final_output)\n",
    "    return final_outputs\n",
    "  else:\n",
    "    final_output = reshape_from_matrix(prev_output, input_shape)\n",
    "    return final_output\n",
    "  \n",
    "def reshape_from_matrix(output_tensor, orig_shape_list):\n",
    "  \"\"\"Reshapes a rank 2 tensor back to its original rank >= 2 tensor.\"\"\"\n",
    "  if len(orig_shape_list) == 2:\n",
    "    return output_tensor\n",
    "\n",
    "  output_shape = get_shape_list(output_tensor)\n",
    "\n",
    "  orig_dims = orig_shape_list[0:-1]\n",
    "  width = output_shape[-1]\n",
    "\n",
    "  return tf.reshape(output_tensor, orig_dims + [width])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BertModel의 생성자로 돌아와서, all_encoder_layers에 transformer_model을 거친 결과물들을 할당한다.<br/>\n",
    "transformer_model에서는 num_hidden_layers 만큼의 self-attention 을 통과시켰다. <br/>\n",
    "all_encoder_layers 는 List[Tensor(8, 128, 768)] 이고 여기서의 길이는 12 이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "      self.all_encoder_layers = transformer_model(\n",
    "                  input_tensor=self.embedding_output,\n",
    "                  attention_mask=attention_mask,\n",
    "                  hidden_size=config.hidden_size,\n",
    "                  num_hidden_layers=config.num_hidden_layers,\n",
    "                  num_attention_heads=config.num_attention_heads,\n",
    "                  intermediate_size=config.intermediate_size,\n",
    "                  intermediate_act_fn=get_activation(config.hidden_act),\n",
    "                  hidden_dropout_prob=config.hidden_dropout_prob,\n",
    "                  attention_probs_dropout_prob=config.attention_probs_dropout_prob,\n",
    "                  initializer_range=config.initializer_range,\n",
    "                  do_return_all_layers=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sequence_output에는 transformer의 맨 마지막 layer를 할당한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "      self.sequence_output = self.all_encoder_layers[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "첫번째 토큰만 빼서 \"pooling\" 시킨다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "      # The \"pooler\" converts the encoded sequence tensor of shape\n",
    "      # [batch_size, seq_length, hidden_size] to a tensor of shape\n",
    "      # [batch_size, hidden_size]. This is necessary for segment-level\n",
    "      # (or segment-pair-level) classification tasks where we need a fixed\n",
    "      # dimensional representation of the segment.\n",
    "      with tf.variable_scope(\"pooler\"):\n",
    "        # We \"pool\" the model by simply taking the hidden state corresponding\n",
    "        # to the first token. We assume that this has been pre-trained\n",
    "        first_token_tensor = tf.squeeze(self.sequence_output[:, 0:1, :], axis=1)\n",
    "        self.pooled_output = tf.layers.dense(\n",
    "            first_token_tensor,\n",
    "            config.hidden_size,\n",
    "            activation=tf.tanh,\n",
    "            kernel_initializer=create_initializer(config.initializer_range))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 BertModel의 생성자가 끝이 났다. 다시 model_fn 으로 돌아가서, BertModel의 sequeuence output으로 masked LM output을 구한다.<br/>\n",
    "parameter는 아래와 같다.\n",
    "\n",
    "```\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    (masked_lm_loss,\n",
    "     masked_lm_example_loss, masked_lm_log_probs) = get_masked_lm_output(\n",
    "         bert_config, model.get_sequence_output(), model.get_embedding_table(),\n",
    "         masked_lm_positions, masked_lm_ids, masked_lm_weights)\n",
    "  \n",
    "def get_masked_lm_output(bert_config, input_tensor, output_weights, positions,\n",
    "                         label_ids, label_weights):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-31 12:33:31,756 :  \n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run_config = tf.contrib.tpu.RunConfig(\n",
    "    model_dir=BERT_GCS_DIR,\n",
    "    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS,\n",
    "    tpu_config=tf.contrib.tpu.TPUConfig(\n",
    "        iterations_per_loop=SAVE_CHECKPOINTS_STEPS,\n",
    "        num_shards=NUM_TPU_CORES,\n",
    "        per_host_input_for_training=tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-31 12:34:11,008 :  Estimator's model_fn (<function model_fn_builder.<locals>.model_fn at 0x12fccbb90>) includes params argument, but params are not passed to Estimator.\n",
      "2019-12-31 12:34:11,024 :  Using config: {'_model_dir': './bert_model', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 2500, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x12fcd4310>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=2500, num_shards=8, num_cores_per_replica=None, per_host_input_for_training=3, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2, experimental_host_call_every_n_steps=1), '_cluster': None}\n",
      "2019-12-31 12:34:11,026 :  _TPUContext: eval_on_tpu True\n",
      "2019-12-31 12:34:11,027 :  eval_on_tpu ignored because use_tpu is False.\n"
     ]
    }
   ],
   "source": [
    "estimator = tf.contrib.tpu.TPUEstimator(\n",
    "    use_tpu=USE_TPU,\n",
    "    model_fn=model_fn,\n",
    "    config=run_config,\n",
    "    train_batch_size=TRAIN_BATCH_SIZE,\n",
    "    eval_batch_size=EVAL_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_fn = input_fn_builder(\n",
    "        input_files=input_files,\n",
    "        max_seq_length=MAX_SEQ_LENGTH,\n",
    "        max_predictions_per_seq=MAX_PREDICTIONS,\n",
    "        is_training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.train(input_fn=train_input_fn, max_steps=TRAIN_STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
