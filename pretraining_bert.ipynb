{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-training BERT\n",
    "preprocess_bert.py 에서 이어짐. <br/>\n",
    "아래 링크에서 step.6 부터\n",
    "https://colab.research.google.com/drive/1nVn6AFpQSzXBt8_ywfx6XR8ZfQXlKGAz#scrollTo=myjxQe5awo1v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "sys.path.append(\"bert\")\n",
    "\n",
    "from bert import modeling, optimization, tokenization\n",
    "from bert.run_pretraining import input_fn_builder, model_fn_builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure logging\n",
    "log = logging.getLogger('tensorflow')\n",
    "log.setLevel(logging.INFO)\n",
    "\n",
    "# create formatter and add it to the handlers\n",
    "formatter = logging.Formatter('%(asctime)s :  %(message)s')\n",
    "sh = logging.StreamHandler()\n",
    "sh.setLevel(logging.INFO)\n",
    "sh.setFormatter(formatter)\n",
    "log.handlers = [sh]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOC_FNAME = \"vocab.txt\" #@param {type:\"string\"}\n",
    "VOC_SIZE = 32000 #@param {type:\"integer\"}\n",
    "USE_TPU=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"bert_resourses\" #@param {type:\"string\"}\n",
    "MODEL_DIR = \"bert_model\" #@param {type:\"string\"}\n",
    "tf.gfile.MkDir(MODEL_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hyperparameter 세팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this for BERT-base\n",
    "\n",
    "bert_base_config = {\n",
    "  \"attention_probs_dropout_prob\": 0.1, \n",
    "  \"directionality\": \"bidi\", \n",
    "  \"hidden_act\": \"gelu\", \n",
    "  \"hidden_dropout_prob\": 0.1, \n",
    "  \"hidden_size\": 768, \n",
    "  \"initializer_range\": 0.02, \n",
    "  \"intermediate_size\": 3072, \n",
    "  \"max_position_embeddings\": 512, \n",
    "  \"num_attention_heads\": 12, \n",
    "  \"num_hidden_layers\": 12, \n",
    "  \"pooler_fc_size\": 768, \n",
    "  \"pooler_num_attention_heads\": 12, \n",
    "  \"pooler_num_fc_layers\": 3, \n",
    "  \"pooler_size_per_head\": 128, \n",
    "  \"pooler_type\": \"first_token_transform\", \n",
    "  \"type_vocab_size\": 2, \n",
    "  \"vocab_size\": VOC_SIZE\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-31 12:32:39,379 :  Using checkpoint: None\n",
      "2019-12-31 12:32:39,385 :  Using 4 data shards\n"
     ]
    }
   ],
   "source": [
    "BUCKET_NAME = \"bert_resourses\" #@param {type:\"string\"}\n",
    "MODEL_DIR = \"bert_model\" #@param {type:\"string\"}\n",
    "PRETRAINING_DIR = \"pretraining_data\" #@param {type:\"string\"}\n",
    "VOC_FNAME = \"vocab.txt\" #@param {type:\"string\"}\n",
    "\n",
    "# Input data pipeline config\n",
    "TRAIN_BATCH_SIZE = 128 #@param {type:\"integer\"}\n",
    "MAX_PREDICTIONS = 20 #@param {type:\"integer\"}\n",
    "MAX_SEQ_LENGTH = 128 #@param {type:\"integer\"}\n",
    "MASKED_LM_PROB = 0.15 #@param\n",
    "\n",
    "# Training procedure config\n",
    "EVAL_BATCH_SIZE = 64\n",
    "LEARNING_RATE = 2e-5\n",
    "TRAIN_STEPS = 1000000 #@param {type:\"integer\"}\n",
    "SAVE_CHECKPOINTS_STEPS = 2500 #@param {type:\"integer\"}\n",
    "NUM_TPU_CORES = 8\n",
    "\n",
    "BUCKET_PATH = \".\"\n",
    "\n",
    "BERT_GCS_DIR = \"{}/{}\".format(BUCKET_PATH, MODEL_DIR)\n",
    "DATA_GCS_DIR = \"{}/{}\".format(BUCKET_PATH, PRETRAINING_DIR)\n",
    "\n",
    "VOCAB_FILE = os.path.join(BERT_GCS_DIR, VOC_FNAME)\n",
    "CONFIG_FILE = os.path.join(BERT_GCS_DIR, \"bert_config.json\")\n",
    "\n",
    "INIT_CHECKPOINT = tf.train.latest_checkpoint(BERT_GCS_DIR)\n",
    "\n",
    "bert_config = modeling.BertConfig.from_json_file(CONFIG_FILE)\n",
    "input_files = tf.gfile.Glob(os.path.join(DATA_GCS_DIR,'*tfrecord'))\n",
    "\n",
    "log.info(\"Using checkpoint: {}\".format(INIT_CHECKPOINT))\n",
    "log.info(\"Using {} data shards\".format(len(input_files)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fn = model_fn_builder(\n",
    "      bert_config=bert_config,\n",
    "      init_checkpoint=INIT_CHECKPOINT,\n",
    "      learning_rate=LEARNING_RATE,\n",
    "      num_train_steps=TRAIN_STEPS,\n",
    "      num_warmup_steps=10,\n",
    "      use_tpu=USE_TPU,\n",
    "      use_one_hot_embeddings=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# model_fn_builder code review\n",
    "bert.run_pretraining.py - model_fn_builder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model_fn_builder는 TPUEstimator를 사용하기 위해 closure를 리턴한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn_builder(bert_config, init_checkpoint, learning_rate,\n",
    "                     num_train_steps, num_warmup_steps, use_tpu,\n",
    "                     use_one_hot_embeddings):\n",
    "  \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n",
    "  \n",
    "  def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n",
    "  \"\"\"The `model_fn` for TPUEstimator.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "features의 값들을 상수에 할당한다. preprocess에서 파일에 썼던 값들을 다시 기록해보면,\n",
    "\n",
    "features 에 쓰인 값들은 아래와 같다.\n",
    "```\n",
    "input_ids = {list: 128} [2, 23, 11, 4, 68, 735, 3, 5, 4, 10213, 294, 735, 9, 577, 449, 4, 3, 0, 0, , ...]\n",
    "input_mask = {list: 128} [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, ...]\n",
    "segment_ids = {list: 128} [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, ...]\n",
    "masked_lm_positions = {list: 20} [3, 8, 15, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "masked_lm_ids = {list: 20} [8, 73, 287, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "masked_lm_ids = {list: 20} [8, 73, 287, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "masked_lm_weights = {list: 20} [1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...]\n",
    "next_sentence_label = {int} 0\n",
    "```\n",
    "\n",
    "각각의 값을 다시 정리해보면,\n",
    "```\n",
    "input_ids: token word의 id (int) 변환값\n",
    "input_mask: input_ids 길이만큼만 1, 나머지 0 (아마도 패딩 여부를 파악하기 위한 것으로 보인다)\n",
    "segment_ids: segment A는 0, B는 1\n",
    "masked_lm_positions: masked word의 위치 index\n",
    "masked_lm_ids: masked word의 id (int) 변환값\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "    input_ids = features[\"input_ids\"]\n",
    "    input_mask = features[\"input_mask\"]\n",
    "    segment_ids = features[\"segment_ids\"]\n",
    "    masked_lm_positions = features[\"masked_lm_positions\"]\n",
    "    masked_lm_ids = features[\"masked_lm_ids\"]\n",
    "    masked_lm_weights = features[\"masked_lm_weights\"]\n",
    "    next_sentence_labels = features[\"next_sentence_labels\"]\n",
    "\n",
    "    is_training = (mode == tf.estimator.ModeKeys.TRAIN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bert model을 정의해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = modeling.BertModel(\n",
    "        config=bert_config,\n",
    "        is_training=is_training,\n",
    "        input_ids=input_ids,\n",
    "        input_mask=input_mask,\n",
    "        token_type_ids=segment_ids,\n",
    "        use_one_hot_embeddings=use_one_hot_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "BertModel 의 생성자를 보면, transformer encoder 라고 되어 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class BertModel(object):\n",
    "  \"\"\"BERT model (\"Bidirectional Encoder Representations from Transformers\").\n",
    "\n",
    "  Example usage:\n",
    "\n",
    "  ```python\n",
    "  # Already been converted into WordPiece token ids\n",
    "  input_ids = tf.constant([[31, 51, 99], [15, 5, 0]])\n",
    "  input_mask = tf.constant([[1, 1, 1], [1, 1, 0]])\n",
    "  token_type_ids = tf.constant([[0, 0, 1], [0, 2, 0]])\n",
    "\n",
    "  config = modeling.BertConfig(vocab_size=32000, hidden_size=512,\n",
    "    num_hidden_layers=8, num_attention_heads=6, intermediate_size=1024)\n",
    "\n",
    "  model = modeling.BertModel(config=config, is_training=True,\n",
    "    input_ids=input_ids, input_mask=input_mask, token_type_ids=token_type_ids)\n",
    "\n",
    "  label_embeddings = tf.get_variable(...)\n",
    "  pooled_output = model.get_pooled_output()\n",
    "  logits = tf.matmul(pooled_output, label_embeddings)\n",
    "  ...\n",
    "  ```\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               config,\n",
    "               is_training,\n",
    "               input_ids,\n",
    "               input_mask=None,\n",
    "               token_type_ids=None,\n",
    "               use_one_hot_embeddings=False,\n",
    "               scope=None):\n",
    "    \"\"\"Constructor for BertModel.\n",
    "\n",
    "    Args:\n",
    "      config: `BertConfig` instance.\n",
    "      is_training: bool. true for training model, false for eval model. Controls\n",
    "        whether dropout will be applied.\n",
    "      input_ids: int32 Tensor of shape [batch_size, seq_length].\n",
    "      input_mask: (optional) int32 Tensor of shape [batch_size, seq_length].\n",
    "      token_type_ids: (optional) int32 Tensor of shape [batch_size, seq_length].\n",
    "      use_one_hot_embeddings: (optional) bool. Whether to use one-hot word\n",
    "        embeddings or tf.embedding_lookup() for the word embeddings.\n",
    "      scope: (optional) variable scope. Defaults to \"bert\".\n",
    "\n",
    "    Raises:\n",
    "      ValueError: The config is invalid or one of the input tensor shapes\n",
    "        is invalid.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "먼저 input의 shape를 구하고"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "    input_shape = get_shape_list(input_ids, expected_rank=2)\n",
    "    batch_size = input_shape[0]\n",
    "    seq_length = input_shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "embedding lookup을 수행하여 준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    with tf.variable_scope(scope, default_name=\"bert\"):\n",
    "      with tf.variable_scope(\"embeddings\"):\n",
    "        # Perform embedding lookup on the word ids.\n",
    "        (self.embedding_output, self.embedding_table) = embedding_lookup(\n",
    "            input_ids=input_ids,\n",
    "            vocab_size=config.vocab_size,\n",
    "            embedding_size=config.hidden_size,\n",
    "            initializer_range=config.initializer_range,\n",
    "            word_embedding_name=\"word_embeddings\",\n",
    "            use_one_hot_embeddings=use_one_hot_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_lookup(input_ids,\n",
    "                     vocab_size,\n",
    "                     embedding_size=128,\n",
    "                     initializer_range=0.02,\n",
    "                     word_embedding_name=\"word_embeddings\",\n",
    "                     use_one_hot_embeddings=False):\n",
    "  \"\"\"Looks up words embeddings for id tensor.\n",
    "\n",
    "  Args:\n",
    "    input_ids: int32 Tensor of shape [batch_size, seq_length] containing word\n",
    "      ids.\n",
    "    vocab_size: int. Size of the embedding vocabulary.\n",
    "    embedding_size: int. Width of the word embeddings.\n",
    "    initializer_range: float. Embedding initialization range.\n",
    "    word_embedding_name: string. Name of the embedding table.\n",
    "    use_one_hot_embeddings: bool. If True, use one-hot method for word\n",
    "      embeddings. If False, use `tf.gather()`.\n",
    "\n",
    "  Returns:\n",
    "    float Tensor of shape [batch_size, seq_length, embedding_size].\n",
    "  \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "먼저 input_ids가 2차원 (batch_size, seq_length) 이라면 차원을 하나 늘려준다.\n",
    "결과적으로 (batch_size, seq_length, 1) 이 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  # This function assumes that the input is of shape [batch_size, seq_length,\n",
    "  # num_inputs].\n",
    "  #\n",
    "  # If the input is a 2D tensor of shape [batch_size, seq_length], we\n",
    "  # reshape to [batch_size, seq_length, 1].\n",
    "  if input_ids.shape.ndims == 2:\n",
    "    input_ids = tf.expand_dims(input_ids, axis=[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hidden size에 맞는 embedding table (tf.Variable) 을 하나 만들어준다.\n",
    "embedding_size는 config에서 준 hidden_size (여기서는 768 이다.)\n",
    "즉 embedding_table은 (32000, 768) 이 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  embedding_table = tf.get_variable(\n",
    "      name=word_embedding_name,\n",
    "      shape=[vocab_size, embedding_size],\n",
    "      initializer=create_initializer(initializer_range))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input_ids를 flat 해서, embedding table에서 input_ids에 해당하는 vector를 가져온다.<br/>\n",
    "여기서 use_one_hot_embedding=True 이므로, input_ids를 곱해준다. <br/>\n",
    "- flat_input_ids = (8 * 128) = (1024,)\n",
    "- one_hot_input_ids = (1024, 32000)\n",
    "- output = (1024, 768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  flat_input_ids = tf.reshape(input_ids, [-1])\n",
    "  if use_one_hot_embeddings:\n",
    "    one_hot_input_ids = tf.one_hot(flat_input_ids, depth=vocab_size)\n",
    "    output = tf.matmul(one_hot_input_ids, embedding_table)\n",
    "  else:\n",
    "    output = tf.gather(embedding_table, flat_input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그 후 output을 input_ids의 shape에 맞게 복원해준다. 이 때 마지막 차원에 embedding_size를 곱해준다.\n",
    "- input_shape = [8, 128, 1]\n",
    "- output = (8, 128, 768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  input_shape = get_shape_list(input_ids)\n",
    "\n",
    "  output = tf.reshape(output,\n",
    "                      input_shape[0:-1] + [input_shape[-1] * embedding_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-31 12:33:31,756 :  \n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run_config = tf.contrib.tpu.RunConfig(\n",
    "    model_dir=BERT_GCS_DIR,\n",
    "    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS,\n",
    "    tpu_config=tf.contrib.tpu.TPUConfig(\n",
    "        iterations_per_loop=SAVE_CHECKPOINTS_STEPS,\n",
    "        num_shards=NUM_TPU_CORES,\n",
    "        per_host_input_for_training=tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-31 12:34:11,008 :  Estimator's model_fn (<function model_fn_builder.<locals>.model_fn at 0x12fccbb90>) includes params argument, but params are not passed to Estimator.\n",
      "2019-12-31 12:34:11,024 :  Using config: {'_model_dir': './bert_model', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 2500, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x12fcd4310>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=2500, num_shards=8, num_cores_per_replica=None, per_host_input_for_training=3, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2, experimental_host_call_every_n_steps=1), '_cluster': None}\n",
      "2019-12-31 12:34:11,026 :  _TPUContext: eval_on_tpu True\n",
      "2019-12-31 12:34:11,027 :  eval_on_tpu ignored because use_tpu is False.\n"
     ]
    }
   ],
   "source": [
    "estimator = tf.contrib.tpu.TPUEstimator(\n",
    "    use_tpu=USE_TPU,\n",
    "    model_fn=model_fn,\n",
    "    config=run_config,\n",
    "    train_batch_size=TRAIN_BATCH_SIZE,\n",
    "    eval_batch_size=EVAL_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_fn = input_fn_builder(\n",
    "        input_files=input_files,\n",
    "        max_seq_length=MAX_SEQ_LENGTH,\n",
    "        max_predictions_per_seq=MAX_PREDICTIONS,\n",
    "        is_training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.train(input_fn=train_input_fn, max_steps=TRAIN_STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
