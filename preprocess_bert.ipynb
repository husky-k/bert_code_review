{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pre-training BERT\n",
    "# 아래 링크의 코드를 그대로 가져옴\n",
    "- https://colab.research.google.com/drive/1nVn6AFpQSzXBt8_ywfx6XR8ZfQXlKGAz#scrollTo=myjxQe5awo1v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow==1.15\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentencepiece\n",
    "!git clone https://github.com/google-research/bert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import & set logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/kmryu/code/deep/bert_code_review/bert/optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import nltk\n",
    "import random\n",
    "import logging\n",
    "import tensorflow as tf\n",
    "import sentencepiece as spm\n",
    "\n",
    "from glob import glob\n",
    "from tensorflow.keras.utils import Progbar\n",
    "\n",
    "sys.path.append(\"bert\")\n",
    "\n",
    "from bert import modeling, optimization, tokenization\n",
    "from bert.run_pretraining import input_fn_builder, model_fn_builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure logging\n",
    "log = logging.getLogger('tensorflow')\n",
    "log.setLevel(logging.INFO)\n",
    "\n",
    "# create formatter and add it to the handlers\n",
    "formatter = logging.Formatter('%(asctime)s :  %(message)s')\n",
    "sh = logging.StreamHandler()\n",
    "sh.setLevel(logging.INFO)\n",
    "sh.setFormatter(formatter)\n",
    "log.handlers = [sh]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_TPU = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget http://opus.nlpl.eu/download.php?f=OpenSubtitles/v2016/mono/OpenSubtitles.raw.en.gz -O dataset.txt.gz\n",
    "!gzip -d dataset.txt.gz\n",
    "!tail dataset.txt\n",
    "\n",
    "DEMO_MODE = True #@param {type:\"boolean\"}\n",
    "\n",
    "if DEMO_MODE:\n",
    "  CORPUS_SIZE = 10000\n",
    "else:\n",
    "  CORPUS_SIZE = 100000000 #@param {type: \"integer\"}\n",
    "  \n",
    "!(head -n $CORPUS_SIZE dataset.txt) > subdataset.txt\n",
    "!mv subdataset.txt dataset.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocess text\n",
    "Remove punctuation, uppercase letters and non-UTF symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_tokenizer = nltk.RegexpTokenizer(\"\\w+\")\n",
    "\n",
    "def normalize_text(text):\n",
    "  # lowercase text\n",
    "  text = str(text).lower()\n",
    "  # remove non-UTF\n",
    "  text = text.encode(\"utf-8\", \"ignore\").decode()\n",
    "  # remove punktuation symbols\n",
    "  text = \" \".join(regex_tokenizer.tokenize(text))\n",
    "  return text\n",
    "\n",
    "def count_lines(filename):\n",
    "  count = 0\n",
    "  with open(filename) as fi:\n",
    "    for line in fi:\n",
    "      count += 1\n",
    "  return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'thanks to the advance they have succeeded in getting over their adversaries'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize_text('Thanks to the advance, they have succeeded in getting over their adversaries.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000/1000000 [==============================] - 7s 7us/step\n"
     ]
    }
   ],
   "source": [
    "RAW_DATA_FPATH = \"dataset.txt\" #@param {type: \"string\"}\n",
    "PRC_DATA_FPATH = \"proc_dataset.txt\" #@param {type: \"string\"}\n",
    "\n",
    "# apply normalization to the dataset\n",
    "# this will take a minute or two\n",
    "\n",
    "total_lines = count_lines(RAW_DATA_FPATH)\n",
    "bar = Progbar(total_lines)\n",
    "\n",
    "with open(RAW_DATA_FPATH,encoding=\"utf-8\") as fi:\n",
    "  with open(PRC_DATA_FPATH, \"w\",encoding=\"utf-8\") as fo:\n",
    "    for l in fi:\n",
    "      fo.write(normalize_text(l)+\"\\n\")\n",
    "      bar.add(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "that s nice\r\n",
      "sounds like the left bank s running lean\r\n",
      "the service department s over there\r\n",
      "can i talk to you\r\n",
      "talk to me\r\n",
      "yeah sure talk\r\n",
      "well thing of it is i came back because\r\n",
      "do you know what time it is\r\n",
      "it s 5 30\r\n",
      "i came back because\r\n"
     ]
    }
   ],
   "source": [
    "!tail proc_dataset.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# building the vocabulary\n",
    "BERT는 WordPiece tokenizer를 사용했으나, open source가 아님. 그래서 대신 SentencePiece tokenizer를 unigram mode로 사용하려 한다. 이건 BERT에 바로 적용이 안되고 몇가지 트릭을 사용해야 한다.\n",
    "SentencePiece는 RAM을 엄청 많이 사용하므로, 바로 돌리면 crash 된다. 따라서 randomly subsample을 돌리기로 한다.\n",
    "그리고 SentencePiece는 BOS, EOS symbol을 자동으로 더해주기 때문에 이를 막기 위해 저 symbol들의 index를 -1로 둔다.\n",
    "NUM_PLACEHOLDERS는 fine-tune을 위해 예비로 남겨두는 자리이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_PREFIX = \"tokenizer\" #@param {type: \"string\"}\n",
    "VOC_SIZE = 32000 #@param {type:\"integer\"}\n",
    "SUBSAMPLE_SIZE = 12800000 #@param {type:\"integer\"}\n",
    "NUM_PLACEHOLDERS = 256 #@param {type:\"integer\"}\n",
    "\n",
    "SPM_COMMAND = ('--input={} --model_prefix={} '\n",
    "               '--vocab_size={} --input_sentence_size={} '\n",
    "               '--shuffle_input_sentence=true ' \n",
    "               '--bos_id=-1 --eos_id=-1').format(\n",
    "               PRC_DATA_FPATH, MODEL_PREFIX, \n",
    "               VOC_SIZE - NUM_PLACEHOLDERS, SUBSAMPLE_SIZE)\n",
    "\n",
    "spm.SentencePieceTrainer.Train(SPM_COMMAND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "testcase = \"Colorless geothermal substations are generating furiously\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    ">>> wordpiece.tokenize(\"Colorless geothermal substations are generating furiously\")\n",
    " \n",
    "['color',\n",
    " '##less',\n",
    " 'geo',\n",
    " '##thermal',\n",
    " 'sub',\n",
    " '##station',\n",
    " '##s',\n",
    " 'are',\n",
    " 'generating',\n",
    " 'furiously']\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위에 보는 대로, wordpiece tokenizer는 subword 들 중에서 중간에 오는 단어들에 '##'을 붙여준다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README.md             proc_dataset.txt      tokenizer.vocab\r\n",
      "\u001b[1m\u001b[36mbert\u001b[m\u001b[m                  requirements.txt      \u001b[1m\u001b[36mvenv\u001b[m\u001b[m\r\n",
      "dataset.txt           \u001b[1m\u001b[36mshards\u001b[m\u001b[m                vocab.txt\r\n",
      "preprocess_bert.ipynb tokenizer.model\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SentencePiece는 두개의 파일을 남긴다.\n",
    "- tokenizer.model \n",
    "- tokenizer.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<unk>\t0\r\n",
      "▁you\t-3.2342\r\n",
      "▁i\t-3.2821\r\n",
      "▁the\t-3.56375\r\n",
      "▁s\t-3.84955\r\n",
      "▁to\t-3.87601\r\n",
      "▁a\t-3.9102\r\n",
      "▁it\t-3.97593\r\n",
      "▁t\t-4.25729\r\n",
      "▁and\t-4.32686\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 10 tokenizer.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learnt vocab size: 31743\n",
      "Sample tokens: ['▁alana', 'potatoes', '▁defend', 'sailing', 'artman', '▁head', 'pb', 'sation', 'nette', '▁serpent']\n"
     ]
    }
   ],
   "source": [
    "def read_sentencepiece_vocab(filepath):\n",
    "  voc = []\n",
    "  with open(filepath, encoding='utf-8') as fi:\n",
    "    for line in fi:\n",
    "      voc.append(line.split(\"\\t\")[0])\n",
    "  # skip the first <unk> token\n",
    "  voc = voc[1:]\n",
    "  return voc\n",
    "\n",
    "snt_vocab = read_sentencepiece_vocab(\"{}.vocab\".format(MODEL_PREFIX))\n",
    "print(\"Learnt vocab size: {}\".format(len(snt_vocab)))\n",
    "print(\"Sample tokens: {}\".format(random.sample(snt_vocab, 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SentencePiece는 WordPiece와 반대로 동작한다는 것을 알 수 있다. SentencePiece는 whitespace를 아래와 같이 \"▁\" (U+2581)로 변경한다.\n",
    "```\n",
    "Hello▁World.\n",
    "```\n",
    "\n",
    "그리고 문장을 쪼갠다.\n",
    "```\n",
    "[Hello] [▁Wor] [ld] [.]\n",
    "```\n",
    "\n",
    "따라서 \"▁\"가 있으면 없애고 아니면 \"##\"을 붙여 주어야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_sentencepiece_token(token):\n",
    "    if token.startswith(\"▁\"):\n",
    "        return token[1:]\n",
    "    else:\n",
    "        return \"##\" + token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_vocab = list(map(parse_sentencepiece_token, snt_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctrl_symbols = [\"[PAD]\",\"[UNK]\",\"[CLS]\",\"[SEP]\",\"[MASK]\"]\n",
    "bert_vocab = ctrl_symbols + bert_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT에 사용되는 문자들과 placeholder token들을 vocab에 더해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32000\n"
     ]
    }
   ],
   "source": [
    "bert_vocab += [\"[UNUSED_{}]\".format(i) for i in range(VOC_SIZE - len(bert_vocab))]\n",
    "print(len(bert_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOC_FNAME = \"vocab.txt\" #@param {type:\"string\"}\n",
    "\n",
    "with open(VOC_FNAME, \"w\") as fo:\n",
    "  for token in bert_vocab:\n",
    "    fo.write(token+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-28 18:19:05,423 :  From /Users/kmryu/code/deep/bert_code_review/bert/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['color',\n",
       " '##less',\n",
       " 'geo',\n",
       " '##ther',\n",
       " '##mal',\n",
       " 'subs',\n",
       " '##tation',\n",
       " '##s',\n",
       " 'are',\n",
       " 'generat',\n",
       " '##ing',\n",
       " 'furious',\n",
       " '##ly']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_tokenizer = tokenization.FullTokenizer(VOC_FNAME)\n",
    "bert_tokenizer.tokenize(testcase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generating pre-training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split: illegal option -- d\r\n",
      "usage: split [-a sufflen] [-b byte_count] [-l line_count] [-p pattern]\r\n",
      "             [file [prefix]]\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir ./shards\n",
    "!split -a 4 -l 256000 -d $PRC_DATA_FPATH ./shards/shard_\n",
    "!ls ./shards/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shard_0000 shard_0001 shard_0002 shard_0003\r\n"
     ]
    }
   ],
   "source": [
    "!ls ./shards/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LENGTH = 128 #@param {type:\"integer\"}\n",
    "MASKED_LM_PROB = 0.15 #@param\n",
    "MAX_PREDICTIONS = 20 #@param {type:\"integer\"}\n",
    "DO_LOWER_CASE = True #@param {type:\"boolean\"}\n",
    "PROCESSES = 2 #@param {type:\"integer\"}\n",
    "PRETRAINING_DIR = \"pretraining_data\" #@param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "XARGS_CMD = (\"ls ./shards/ | \"\n",
    "             \"xargs -n 1 -P {} -I{} \"\n",
    "             \"python3 bert/create_pretraining_data.py \"\n",
    "             \"--input_file=./shards/{} \"\n",
    "             \"--output_file={}/{}.tfrecord \"\n",
    "             \"--vocab_file={} \"\n",
    "             \"--do_lower_case={} \"\n",
    "             \"--max_predictions_per_seq={} \"\n",
    "             \"--max_seq_length={} \"\n",
    "             \"--masked_lm_prob={} \"\n",
    "             \"--random_seed=34 \"\n",
    "             \"--dupe_factor=5\")\n",
    "\n",
    "XARGS_CMD = XARGS_CMD.format(PROCESSES, '{}', '{}', PRETRAINING_DIR, '{}', \n",
    "                             VOC_FNAME, DO_LOWER_CASE, \n",
    "                             MAX_PREDICTIONS, MAX_SEQ_LENGTH, MASKED_LM_PROB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.gfile.MkDir(PRETRAINING_DIR)\n",
    "!$XARGS_CMD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# create_pretraining_data 코드 리뷰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ls ./shards/ | xargs -n 1 -P 2 -I{} python3 bert/create_pretraining_data.py --input_file=./shards/{} --output_file=pretraining_data/{}.tfrecord --vocab_file=vocab.txt --do_lower_case=True --max_predictions_per_seq=20 --max_seq_length=128 --masked_lm_prob=0.15 --random_seed=34 --dupe_factor=5'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XARGS_CMD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags = tf.flags\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "flags.DEFINE_string(\"input_file\", None,\n",
    "                    \"Input raw text file (or comma-separated list of files).\")\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    \"output_file\", None,\n",
    "    \"Output TF example file (or comma-separated list of files).\")\n",
    "\n",
    "flags.DEFINE_string(\"vocab_file\", None,\n",
    "                    \"The vocabulary file that the BERT model was trained on.\")\n",
    "\n",
    "flags.DEFINE_bool(\n",
    "    \"do_lower_case\", True,\n",
    "    \"Whether to lower case the input text. Should be True for uncased \"\n",
    "    \"models and False for cased models.\")\n",
    "\n",
    "flags.DEFINE_bool(\n",
    "    \"do_whole_word_mask\", False,\n",
    "    \"Whether to use whole word masking rather than per-WordPiece masking.\")\n",
    "\n",
    "flags.DEFINE_integer(\"max_seq_length\", 128, \"Maximum sequence length.\")\n",
    "\n",
    "flags.DEFINE_integer(\"max_predictions_per_seq\", 20,\n",
    "                     \"Maximum number of masked LM predictions per sequence.\")\n",
    "\n",
    "flags.DEFINE_integer(\"random_seed\", 12345, \"Random seed for data generation.\")\n",
    "\n",
    "flags.DEFINE_integer(\n",
    "    \"dupe_factor\", 10,\n",
    "    \"Number of times to duplicate the input data (with different masks).\")\n",
    "\n",
    "flags.DEFINE_float(\"masked_lm_prob\", 0.15, \"Masked LM probability.\")\n",
    "\n",
    "flags.DEFINE_float(\n",
    "    \"short_seq_prob\", 0.1,\n",
    "    \"Probability of creating sequences which are shorter than the \"\n",
    "    \"maximum length.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FLAGS는 tf의 command line options를 담아놓는 객체로 보인다.\n",
    "debugging으로 확인해보면 위의 옵션들은 다음과 같다.\n",
    "```\n",
    "input_file = {str} './shards/shard_0000'\n",
    "output_file = {str} 'pretraining_data/shard_0000.tfrecord'\n",
    "vocab_file = {str} 'vocab.txt'\n",
    "\n",
    "do_lower_case = {bool} True\n",
    "do_whole_word_mask = {bool} False\n",
    "max_seq_length = {int} 128\n",
    "max_predictions_per_seq = {int} 20\n",
    "\n",
    "dupe_factor = {int} 5\n",
    "masked_lm_prob = {float} 0.15\n",
    "short_seq_prob = {float} 0.1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "main 함수는 로깅을 제외하면 4가지로 볼 수 있다.\n",
    "- tokenizer 선언\n",
    "- input_files 선언\n",
    "- create_training_instances\n",
    "- write_instances_to_example_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(_):\n",
    "  tokenizer = tokenization.FullTokenizer(\n",
    "    vocab_file=FLAGS.vocab_file,\n",
    "    do_lower_case=FLAGS.do_lower_case\n",
    "  )\n",
    "  \n",
    "  input_files = []\n",
    "  for input_pattern in FLAGS.input_file.split(\",\"):\n",
    "    input_files.extend(tf.gfile.Glob(input_pattern))\n",
    "    \n",
    "  instances = create_training_instances(\n",
    "      input_files, tokenizer, FLAGS.max_seq_length, FLAGS.dupe_factor,\n",
    "      FLAGS.short_seq_prob, FLAGS.masked_lm_prob, FLAGS.max_predictions_per_seq,\n",
    "      rng)\n",
    "  \n",
    "  write_instance_to_example_files(instances, tokenizer, FLAGS.max_seq_length,\n",
    "                                  FLAGS.max_predictions_per_seq, output_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기서 tokenization은 bert의 tokenization.py 이다.\n",
    "위에서 이 tokenization을 사용해서 test를 했었다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['color',\n",
       " '##less',\n",
       " 'geo',\n",
       " '##ther',\n",
       " '##mal',\n",
       " 'subs',\n",
       " '##tation',\n",
       " '##s',\n",
       " 'are',\n",
       " 'generat',\n",
       " '##ing',\n",
       " 'furious',\n",
       " '##ly']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = tokenization.FullTokenizer(VOC_FNAME)\n",
    "tokenizer.tokenize(testcase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input_files는 command line option 에서 받은 그 파일들을 말한다.\n",
    "아래 코드의 결과물은 다음과 같다.\n",
    "```\n",
    "input_files = <class 'list'>: ['../shards/shard_0000']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_files = []\n",
    "  for input_pattern in FLAGS.input_file.split(\",\"):\n",
    "    input_files.extend(tf.gfile.Glob(input_pattern))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create_training_instances 에서는 input_files 에서 한줄씩 읽어서 training instance로 변환한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances = create_training_instances(\n",
    "      input_files, tokenizer, FLAGS.max_seq_length, FLAGS.dupe_factor,\n",
    "      FLAGS.short_seq_prob, FLAGS.masked_lm_prob, FLAGS.max_predictions_per_seq,\n",
    "      rng)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "먼저 문서들을 token들로 변환해 list of lists로 변환한다.\n",
    "여기서 빈 line이 나오면 문서가 끝이 났다고 생각한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_documents = [[]]\n",
    "\n",
    "  # Input file format:\n",
    "  # (1) One sentence per line. These should ideally be actual sentences, not\n",
    "  # entire paragraphs or arbitrary spans of text. (Because we use the\n",
    "  # sentence boundaries for the \"next sentence prediction\" task).\n",
    "  # (2) Blank lines between documents. Document boundaries are needed so\n",
    "  # that the \"next sentence prediction\" task doesn't span between documents.\n",
    "  for input_file in input_files:\n",
    "    with tf.gfile.GFile(input_file, \"r\") as reader:\n",
    "      while True:\n",
    "        line = tokenization.convert_to_unicode(reader.readline())\n",
    "        if not line:\n",
    "          break\n",
    "        line = line.strip()\n",
    "\n",
    "        # Empty lines are used as document delimiters\n",
    "        if not line:\n",
    "          all_documents.append([])\n",
    "        tokens = tokenizer.tokenize(line)\n",
    "        if tokens:\n",
    "          all_documents[-1].append(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그 후 문서들을 돌면서 instance로 변환한다. dupe_factor만큼 반복한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_words = list(tokenizer.vocab.keys())\n",
    "instances = []\n",
    "for _ in range(dupe_factor):\n",
    "  for document_index in range(len(all_documents)):\n",
    "    instances.extend(\n",
    "        create_instances_from_document(\n",
    "            all_documents, document_index, max_seq_length, short_seq_prob,\n",
    "            masked_lm_prob, max_predictions_per_seq, vocab_words, rng))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create_instances_from_document 에서 본격적으로 document (tokens)를 instance로 만든다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_instances_from_document(\n",
    "    all_documents, document_index, max_seq_length, short_seq_prob,\n",
    "    masked_lm_prob, max_predictions_per_seq, vocab_words, rng):\n",
    "  \"\"\"Creates `TrainingInstance`s for a single document.\"\"\"\n",
    "  document = all_documents[document_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기서 document는 list of lists 이다.\n",
    "```\n",
    "000 = {list} <class 'list'>: ['and', 'stimulate', 'ying']\n",
    "001 = {list} <class 'list'>: ['your', 'ying']\n",
    "002 = {list} <class 'list'>: ['you', 're', 'crazy']\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "먼저 max_num_tokens를 정하고,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  # Account for [CLS], [SEP], [SEP]\n",
    "  max_num_tokens = max_seq_length - 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "target_seq_length를 정하는데, <br/>\n",
    "fine-tuning에서 짧은 문장도 학습 가능하도록 일정 확률 (short_seq_prob) 만큼은 target_seq_length를 랜덤하게 짧게 만든다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We *usually* want to fill up the entire sequence since we are padding\n",
    "  # to `max_seq_length` anyways, so short sequences are generally wasted\n",
    "  # computation. However, we *sometimes*\n",
    "  # (i.e., short_seq_prob == 0.1 == 10% of the time) want to use shorter\n",
    "  # sequences to minimize the mismatch between pre-training and fine-tuning.\n",
    "  # The `target_seq_length` is just a rough target however, whereas\n",
    "  # `max_seq_length` is a hard limit.\n",
    "  target_seq_length = max_num_tokens\n",
    "  if rng.random() < short_seq_prob:\n",
    "    target_seq_length = rng.randint(2, max_num_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그 다음에 문장 두개를 이어 붙이는데, <br/>\n",
    "여기서 문서의 token들을 다 이어 붙인 후 아무 곳이나 정해서 두개의 segment A,B로 나누는 방식을 사용하지는 않는다. <br/>\n",
    "왜냐하면 그렇게 하면 다음 문장 예측 (next sentence prediction) 이 너무 쉬워지기 때문이다. <br/>\n",
    "따라서 사용자가 입력으로 넣어준 '진짜' 문장을 segment로 사용해 segment A,B로 나눈다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances = []\n",
    "current_chunk = []\n",
    "current_length = 0\n",
    "i = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "current_length로 segment 두개의 길이가 최대 길이 (target_seq_length)가 넘지 않도록 막는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while i < len(document):\n",
    "  segment = document[i]\n",
    "  current_chunk.append(segment)\n",
    "  current_length += len(segment)\n",
    "  \n",
    "  if i == len(document) - 1 or current_length >= target_seq_length:\n",
    "    if current_chunk:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "segment A의 길이를 랜덤으로 구한 다음, 그만큼의 토큰만 tokens_a에 붙인다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "      a_end = 1\n",
    "      if len(current_chunk) >= 2:\n",
    "        a_end = rng.randint(1, len(current_chunk) - 1)\n",
    "\n",
    "      tokens_a = []\n",
    "      for j in range(a_end):\n",
    "        tokens_a.extend(current_chunk[j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tokens_b를 구할 때는 50%는 다음 문장, 50%는 랜덤한 문장을 가져온다.<br/>\n",
    "이렇게 함으로써 Next Sentence Prediction을 binary classification 방식으로 하게 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "      if len(current_chunk) == 1 or rng.random() < 0.5:\n",
    "        is_random_next = True\n",
    "        target_b_length = target_seq_length - len(tokens_a)\n",
    "\n",
    "        # This should rarely go for more than one iteration for large\n",
    "        # corpora. However, just to be careful, we try to make sure that\n",
    "        # the random document is not the same as the document\n",
    "        # we're processing.\n",
    "        for _ in range(10):\n",
    "          random_document_index = rng.randint(0, len(all_documents) - 1)\n",
    "          if random_document_index != document_index:\n",
    "            break\n",
    "\n",
    "        random_document = all_documents[random_document_index]\n",
    "        random_start = rng.randint(0, len(random_document) - 1)\n",
    "        for j in range(random_start, len(random_document)):\n",
    "          tokens_b.extend(random_document[j])\n",
    "          if len(tokens_b) >= target_b_length:\n",
    "            break\n",
    "        # We didn't actually use these segments so we \"put them back\" so\n",
    "        # they don't go to waste.\n",
    "        num_unused_segments = len(current_chunk) - a_end\n",
    "        i -= num_unused_segments\n",
    "      # Actual next\n",
    "      else:\n",
    "        is_random_next = False\n",
    "        for j in range(a_end, len(current_chunk)):\n",
    "          tokens_b.extend(current_chunk[j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그 후 tokens_a, tokens_b의 합이 max_num_tokens를 넘지 않게 잘라낸다.<br/>\n",
    "잘라낼 때는 50%확률로 앞에서 잘라낸다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)\n",
    "\n",
    "def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng):\n",
    "  \"\"\"Truncates a pair of sequences to a maximum sequence length.\"\"\"\n",
    "  while True:\n",
    "    total_length = len(tokens_a) + len(tokens_b)\n",
    "    if total_length <= max_num_tokens:\n",
    "      break\n",
    "\n",
    "    trunc_tokens = tokens_a if len(tokens_a) > len(tokens_b) else tokens_b\n",
    "    assert len(trunc_tokens) >= 1\n",
    "\n",
    "    # We want to sometimes truncate from the front and sometimes from the\n",
    "    # back to add more randomness and avoid biases.\n",
    "    if rng.random() < 0.5:\n",
    "      del trunc_tokens[0]\n",
    "    else:\n",
    "      trunc_tokens.pop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그 후 문장 맨 앞에는 '[CLS]', a 문장 뒤와 b문장 뒤에는 각각 '[SEP]'를 붙이고,<br/>\n",
    "tokens_a에는 segment id 0을, tokens_b에는 segment id 1을 붙인 후 <br/>\n",
    "전체를 이어 붙인다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "segment_ids = []\n",
    "tokens.append(\"[CLS]\")\n",
    "segment_ids.append(0)\n",
    "for token in tokens_a:\n",
    "  tokens.append(token)\n",
    "  segment_ids.append(0)\n",
    "\n",
    "tokens.append(\"[SEP]\")\n",
    "segment_ids.append(0)\n",
    "\n",
    "for token in tokens_b:\n",
    "  tokens.append(token)\n",
    "  segment_ids.append(1)\n",
    "tokens.append(\"[SEP]\")\n",
    "segment_ids.append(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그리고 랜덤하게 마스크를 씌운다. <br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(tokens, masked_lm_positions,\n",
    "         masked_lm_labels) = create_masked_lm_predictions(\n",
    "             tokens, masked_lm_prob, max_predictions_per_seq, vocab_words, rng)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기서 논문의 3.1 Pre-train BERT Task #1: Masked LM의 내용과 같지만 차이가 있는데,<br/>\n",
    "바로 Whole Word Masking을 한다는 것이다. <br/>\n",
    "<br/>\n",
    "wordpiece 방식으로 tokenizing을 하면 단어가 쪼개지는데, 쪼개진 단어 조각이 아니라 단어 전체를 한 단위로 보고 masking을 수행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masked_lm_predictions(tokens, masked_lm_prob,\n",
    "                                 max_predictions_per_seq, vocab_words, rng):\n",
    "  \"\"\"Creates the predictions for the masked LM objective.\"\"\"\n",
    "\n",
    "  cand_indexes = []\n",
    "  for (i, token) in enumerate(tokens):\n",
    "    if token == \"[CLS]\" or token == \"[SEP]\":\n",
    "      continue\n",
    "    # Whole Word Masking means that if we mask all of the wordpieces\n",
    "    # corresponding to an original word. When a word has been split into\n",
    "    # WordPieces, the first token does not have any marker and any subsequence\n",
    "    # tokens are prefixed with ##. So whenever we see the ## token, we\n",
    "    # append it to the previous set of word indexes.\n",
    "    #\n",
    "    # Note that Whole Word Masking does *not* change the training code\n",
    "    # at all -- we still predict each WordPiece independently, softmaxed\n",
    "    # over the entire vocabulary.\n",
    "    if (FLAGS.do_whole_word_mask and len(cand_indexes) >= 1 and\n",
    "        token.startswith(\"##\")):\n",
    "      cand_indexes[-1].append(i)\n",
    "    else:\n",
    "      cand_indexes.append([i])\n",
    "      \n",
    "  rng.shuffle(cand_indexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tokens는 단어의 list 이다.\n",
    "```\n",
    "000 = {str} '[CLS]'\n",
    "001 = {str} 'and'\n",
    "002 = {str} 'stimulate'\n",
    "003 = {str} 'ying'\n",
    "004 = {str} 'your'\n",
    "005 = {str} 'ying'\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sequence 마다 몇개를 masking 할지 max_predictions_per_seq, masked_lm_prob를 보고 결정한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  output_tokens = list(tokens)\n",
    "\n",
    "  num_to_predict = min(max_predictions_per_seq,\n",
    "                       max(1, int(round(len(tokens) * masked_lm_prob))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "covered_indexes는 같은 단어가 두번 masking 대상이 되지 않도록 하기 위한 cache 이다.<br/>\n",
    "masking 결과를 masked_lms 리스트에 담는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  masked_lms = []\n",
    "  covered_indexes = set()\n",
    "  for index_set in cand_indexes:\n",
    "    if len(masked_lms) >= num_to_predict:\n",
    "      break\n",
    "    # If adding a whole-word mask would exceed the maximum number of\n",
    "    # predictions, then just skip this candidate.\n",
    "    if len(masked_lms) + len(index_set) > num_to_predict:\n",
    "      continue\n",
    "    is_any_index_covered = False\n",
    "    for index in index_set:\n",
    "      if index in covered_indexes:\n",
    "        is_any_index_covered = True\n",
    "        break\n",
    "    if is_any_index_covered:\n",
    "      continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "논문 3.1 #1 Masked LM 에서 기술한대로, 80%는 '[MASK]'로, 10%는 원래 그대로, 10%는 랜덤한 단어로 masking을 수행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    for index in index_set:\n",
    "      covered_indexes.add(index)\n",
    "\n",
    "      masked_token = None\n",
    "      # 80% of the time, replace with [MASK]\n",
    "      if rng.random() < 0.8:\n",
    "        masked_token = \"[MASK]\"\n",
    "      else:\n",
    "        # 10% of the time, keep original\n",
    "        if rng.random() < 0.5:\n",
    "          masked_token = tokens[index]\n",
    "        # 10% of the time, replace with random word\n",
    "        else:\n",
    "          masked_token = vocab_words[rng.randint(0, len(vocab_words) - 1)]\n",
    "\n",
    "      output_tokens[index] = masked_token\n",
    "\n",
    "      masked_lms.append(MaskedLmInstance(index=index, label=tokens[index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MaskedLmInstance = collections.namedtuple(\"MaskedLmInstance\",\n",
    "                                          [\"index\", \"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "masking을 index로 정렬한 후, masking position과 label들을 리턴해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  masked_lms = sorted(masked_lms, key=lambda x: x.index)\n",
    "\n",
    "  masked_lm_positions = []\n",
    "  masked_lm_labels = []\n",
    "  for p in masked_lms:\n",
    "    masked_lm_positions.append(p.index)\n",
    "    masked_lm_labels.append(p.label)\n",
    "\n",
    "  return (output_tokens, masked_lm_positions, masked_lm_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create_instances_from_document로 다시 돌아오면,\n",
    "```\n",
    "(tokens, masked_lm_positions,\n",
    "         masked_lm_labels) = create_masked_lm_predictions(\n",
    "             tokens, masked_lm_prob, max_predictions_per_seq, vocab_words, rng)\n",
    "```\n",
    "\n",
    "위 함수의 실행 결과는 다음과 같다.\n",
    "\n",
    "```\n",
    "tokens = {list} <class 'list'>: ['[CLS]', 'understood', 'stimulate', 'ying', 'your', 'ying', 'you', 're', 'crazy', 'jallel', 'calm', 'down', 'everything', 'will', 'be', 'okay', 'fight', 'jallel', '[SEP]', 'jallel', 'can', 'you', 'hear', 'me', 'arise', 'easy', '[MASK]', 'it', 'up', 'can', 'articulate', 'stand', 'up', 'it', 's', 'okay', '[MASK]', 'take', 'a', 'deep', '[MASK]', 'jallel', 'go', 'to', 'the', 'hospital', 'but', ...\n",
    "```\n",
    "\n",
    "\n",
    "```\n",
    "masked_lm_positions = {list} <class 'list'>: [1, 26, 30, 36, 40, 47, 51, 56, 61, 77, 87, 90, 97, 100, 101, 109, 112, 115, 120]\n",
    "masked_lm_labels = {list} <class 'list'>: ['and', 'pick', 'you', 'easy', 'breath', 'no', 'it', 'going', 'put', 'not', '##s', 'that', 'that', 'come', 'mr', 'hello', 'is', 'you', 'bucks']\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 정보들로 TrainingInstance 객체를 만들어준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        instance = TrainingInstance(\n",
    "            tokens=tokens,\n",
    "            segment_ids=segment_ids,\n",
    "            is_random_next=is_random_next,\n",
    "            masked_lm_positions=masked_lm_positions,\n",
    "            masked_lm_labels=masked_lm_labels)\n",
    "        instances.append(instance)\n",
    "    ...\n",
    "    return instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingInstance(object):\n",
    "  \"\"\"A single training instance (sentence pair).\"\"\"\n",
    "\n",
    "  def __init__(self, tokens, segment_ids, masked_lm_positions, masked_lm_labels,\n",
    "               is_random_next):\n",
    "    self.tokens = tokens\n",
    "    self.segment_ids = segment_ids\n",
    "    self.is_random_next = is_random_next\n",
    "    self.masked_lm_positions = masked_lm_positions\n",
    "    self.masked_lm_labels = masked_lm_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create_training_instances 로 돌아가면,\n",
    "```\n",
    "  for _ in range(dupe_factor):\n",
    "    for document_index in range(len(all_documents)):\n",
    "      instances.extend(\n",
    "          create_instances_from_document(\n",
    "              all_documents, document_index, max_seq_length, short_seq_prob,\n",
    "              masked_lm_prob, max_predictions_per_seq, vocab_words, rng))\n",
    "\n",
    "  rng.shuffle(instances)\n",
    "  return instances\n",
    "```\n",
    "\n",
    "이와 같이 training_instances를 리턴해준다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 main(_)으로 돌아가면, instance를 전부 만들었다.\n",
    "```\n",
    "instances = create_training_instances(\n",
    "      input_files, tokenizer, FLAGS.max_seq_length, FLAGS.dupe_factor,\n",
    "      FLAGS.short_seq_prob, FLAGS.masked_lm_prob, FLAGS.max_predictions_per_seq,\n",
    "      rng)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "option에서 받은 대로 output_files를 정의한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  output_files = FLAGS.output_file.split(\",\")\n",
    "  tf.logging.info(\"*** Writing to output files ***\")\n",
    "  for output_file in output_files:\n",
    "    tf.logging.info(\"  %s\", output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그리고 파일에 쓴다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_instance_to_example_files(instances, tokenizer, FLAGS.max_seq_length,\n",
    "                                  FLAGS.max_predictions_per_seq, output_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "먼저 output_files 경로들에 TFRecordWriter를 정의하고"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_instance_to_example_files(instances, tokenizer, max_seq_length,\n",
    "                                    max_predictions_per_seq, output_files):\n",
    "  \"\"\"Create TF example files from `TrainingInstance`s.\"\"\"\n",
    "  writers = []\n",
    "  for output_file in output_files:\n",
    "    writers.append(tf.python_io.TFRecordWriter(output_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "token을 id로 변환, id, mask, segment_id들의 리스트를 만든다.<br/>\n",
    "이 때 max_seq_length 만큼만 가져온다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  for (inst_index, instance) in enumerate(instances):\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(instance.tokens)\n",
    "    input_mask = [1] * len(input_ids)\n",
    "    segment_ids = list(instance.segment_ids)\n",
    "    assert len(input_ids) <= max_seq_length\n",
    "\n",
    "    while len(input_ids) < max_seq_length:\n",
    "      input_ids.append(0)\n",
    "      input_mask.append(0)\n",
    "      segment_ids.append(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "masked positions, ids, weights도 위와 비슷하게 변환한다. <br/>\n",
    "max_predictions_per_seq 보다 짧으면 0으로 패딩한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    masked_lm_positions = list(instance.masked_lm_positions)\n",
    "    masked_lm_ids = tokenizer.convert_tokens_to_ids(instance.masked_lm_labels)\n",
    "    masked_lm_weights = [1.0] * len(masked_lm_ids)\n",
    "\n",
    "    while len(masked_lm_positions) < max_predictions_per_seq:\n",
    "      masked_lm_positions.append(0)\n",
    "      masked_lm_ids.append(0)\n",
    "      masked_lm_weights.append(0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "next sentence가 random인지 아닌지를 label로 표시한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    next_sentence_label = 1 if instance.is_random_next else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위에서 구한 parameter들을 dict에 담는다. <br/>\n",
    "create_int_feature는 int list를 tf의 Int64List로 변환한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    features = collections.OrderedDict()\n",
    "    features[\"input_ids\"] = create_int_feature(input_ids)\n",
    "    features[\"input_mask\"] = create_int_feature(input_mask)\n",
    "    features[\"segment_ids\"] = create_int_feature(segment_ids)\n",
    "    features[\"masked_lm_positions\"] = create_int_feature(masked_lm_positions)\n",
    "    features[\"masked_lm_ids\"] = create_int_feature(masked_lm_ids)\n",
    "    features[\"masked_lm_weights\"] = create_float_feature(masked_lm_weights)\n",
    "    features[\"next_sentence_labels\"] = create_int_feature([next_sentence_label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_int_feature(values):\n",
    "  feature = tf.train.Feature(int64_list=tf.train.Int64List(value=list(values)))\n",
    "  return feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "features (dict)를 tf.train.Example로 변경한 뒤, serialize 해서 파일에 쓴다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    tf_example = tf.train.Example(features=tf.train.Features(feature=features))\n",
    "\n",
    "    writers[writer_index].write(tf_example.SerializeToString())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
